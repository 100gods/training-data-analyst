{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple DNN model built in Keras.\n",
    "\n",
    "Let's start off with the Python imports that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating the CSV files\n",
    "\n",
    "We will start with the CSV files that we wrote out in the [first notebook](../01_explore/taxifare.iypnb) of this sequence. Just so you don't have to run the notebook, we saved a copy in ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter  85978 Jun  1 18:03 ../data/taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 402560 Jun  1 18:03 ../data/taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter  85797 Jun  1 18:03 ../data/taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these cells in the [third notebook](../03_tfdata/input_pipeline.ipynb) of this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ['fare_amount', \n",
    "                'pickup_longitude', 'pickup_latitude', \n",
    "                'dropoff_longitude', 'dropoff_latitude', \n",
    "                'passenger_count', 'key']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],['na']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "# load the training data\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "  dataset = (tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "             .map(features_and_labels) # features, label\n",
    "             .cache())\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "  dataset = dataset.prefetch(1) # take advantage of multi-threading; 1=AUTOTUNE\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_10 (DenseFeature (None, 5)            0           scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 64)           384         dense_features_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 8)            520         dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            9           dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 913\n",
      "Trainable params: 913\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Build a simple Keras DNN using its Functional API\n",
    "def transform(inputs):\n",
    "    preproc = inputs.copy()\n",
    "    # scale the lat, lon values to be in -1, 1\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        preproc[lon_col] = tf.keras.layers.Lambda(lambda x: x/180.0, name='scale_{}'.format(lon_col))(inputs[lon_col])\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        preproc[lat_col] = tf.keras.layers.Lambda(lambda x: x/90.0, name='scale_{}'.format(lat_col))(inputs[lat_col])\n",
    "    return preproc\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))) \n",
    "\n",
    "def build_dnn_model():\n",
    "    INPUT_COLS = ['pickup_longitude', 'pickup_latitude', \n",
    "                  'dropoff_longitude', 'dropoff_latitude', \n",
    "                  'passenger_count']\n",
    "\n",
    "    # input layer\n",
    "    inputs = {\n",
    "        colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "           for colname in INPUT_COLS\n",
    "    }\n",
    "    \n",
    "    preproc = transform(inputs)\n",
    "    \n",
    "    # the DenseFeatures layer takes all the inputs and creates a single input layer\n",
    "    feature_columns = [\n",
    "        tf.feature_column.numeric_column(colname)\n",
    "           for colname in INPUT_COLS\n",
    "    ]\n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(feature_columns)(preproc)\n",
    "\n",
    "    # two hidden layers of [64, 8] just in like the BQML DNN\n",
    "    h1 = tf.keras.layers.Dense(64, activation='relu')(dnn_inputs)\n",
    "    h2 = tf.keras.layers.Dense(8, activation='relu')(h1)\n",
    "    # final output would normally have a linear activation because this is regression\n",
    "    # However, we know something about the taxi problem (fares are between $2.5 and below $60).\n",
    "    # Use that here. (You can verify by running this query):\n",
    "    # SELECT APPROX_QUANTILES(fare_amount, 100) FROM serverlessml.cleaned_training_data\n",
    "    fare_thresh = lambda x: tf.keras.activations.relu(x, max_value=60, threshold=2.5)\n",
    "    output = tf.keras.layers.Dense(1, activation=fare_thresh)(h2)\n",
    "    model = tf.keras.models.Model(inputs, output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse, 'mse'])\n",
    "    return model\n",
    "\n",
    "model = build_dnn_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "To train the model, call model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "310/312 [============================>.] - ETA: 0s - loss: 221.6176 - rmse: 14.5495 - mse: 221.6176"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5 # remember the training dataset repeats, so we can specify anything here\n",
    "NUM_EVALS = 5  # evaluate this many times\n",
    "\n",
    "trainds = load_dataset('../data/taxi-train*', TRAIN_BATCH_SIZE, tf.estimator.ModeKeys.TRAIN)\n",
    "evalds = load_dataset('../data/taxi-valid*', 1024, tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "history = model.fit(trainds, \n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EVALS, \n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will improve this model through feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
