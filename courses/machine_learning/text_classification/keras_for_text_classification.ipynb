{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Keras\n",
    "\n",
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "* Learn how to use TF-Hub for transfer learning\n",
    "* Learn how to create a sentence level text classification model using Keras\n",
    "* Learn how to create a word level text classification model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that we have the right version of Tensorflow installed.\n",
    "!pip freeze | grep tf-nightly-2.0-preview || pip install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'dherin-sandbox'\n",
    "PROJECT = 'dherin-sandbox'\n",
    "REGION = 'us-central1'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Strongly Recommended\n",
    "\n",
    "This entire notebook will run in under 10 minutes using a V100 GPU, but will take about 3 hours on CPU\n",
    "\n",
    "You can add a GPU to your AI Platform Notebook instance following [these instructions](https://cloud.google.com/ml-engine/docs/notebooks/manage-hardware-accelerators).  You can remove the GPU after completing the lab (to manage costs).\n",
    "\n",
    "After adding the subsequent cell should print \"GPU Enabled: True\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU Enabled: {}'.format(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    url, title, score\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    LENGTH(title) > 10\n",
    "    AND score > 10\n",
    "    AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    COUNT(title) AS num_articles\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "    source\n",
    "ORDER BY num_articles DESC\n",
    "  LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "\n",
    "regex = '.*://(.[^/]+)/'\n",
    "\n",
    "\n",
    "sub_query = \"\"\"\n",
    "SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '{0}'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '{0}'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "\"\"\".format(regex)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT source,\n",
    "       LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title\n",
    "FROM\n",
    "  ({sub_query})\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\".format(sub_query=sub_query)\n",
    "\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(\n",
    "    query + \" AND MOD(ABS(FARM_FINGERPRINT(title)), 4) > 0\"\n",
    ").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf = bq.query(\n",
    "    query + \"  AND MOD(ABS(FARM_FINGERPRINT(title)), 4) = 0\"\n",
    ").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '../data/txtcls'\n",
    "\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATADIR, 'train.tsv')\n",
    "EVAL_PATH = os.path.join(DATADIR, 'eval.tsv')\n",
    "\n",
    "traindf.to_csv(\n",
    "    TRAIN_PATH, header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "\n",
    "evaldf.to_csv(\n",
    "    EVAL_PATH, header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 $TRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l $TRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l $EVAL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Level Model with DNN\n",
    "\n",
    "Now that we have our dataset, we need to represent our text data numerically. [Tensorflow Hub](https://www.tensorflow.org/hub) makes this super easy. It contains a library of pre-trained text embeddings that we can download and use with a few lines of code. \n",
    "\n",
    "In particular we will use [this](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1) embedding which encodes sentences into 128 dimensional vectors.\n",
    "\n",
    "Once we have the embedded representation we can simply feed it through a DNN for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {\n",
    "    'github': 0,\n",
    "    'nytimes': 1,\n",
    "    'techcrunch': 2,\n",
    "}\n",
    "\n",
    "N_CLASSES = len(CLASSES)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "HUB = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "TRAIN = tf.estimator.ModeKeys.TRAIN\n",
    "EVAL = tf.estimator.ModeKeys.EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    column_names = ('label', 'text')\n",
    "    \n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "    \n",
    "    X_train = list(df_train['text'])\n",
    "    Y_train = np.array(df_train['label'].map(CLASSES))\n",
    "    X_test = list(df_eval['text'])\n",
    "    Y_test = np.array(df_eval['label'].map(CLASSES))\n",
    "    \n",
    "    return (X_train, Y_train), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_hacker_news_data(\n",
    "    TRAIN_PATH, EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE = 1\n",
    "\n",
    "print(\"X_train:\", X_train[EXAMPLE])\n",
    "print(\"Y_train:\", Y_train[EXAMPLE])\n",
    "\n",
    "assert Y_train[EXAMPLE] in CLASSES.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts, labels, batch_size, mode):\n",
    "    # Precision and recall metrics require one hot labels\n",
    "    labels = tf.one_hot(labels, N_CLASSES)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else:\n",
    "        return dataset.shuffle(50000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(X_train, Y_train, BATCH_SIZE, mode=TRAIN)\n",
    "\n",
    "eval_dataset = create_dataset(X_test, Y_test, BATCH_SIZE, mode=EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    assert x.shape == (BATCH_SIZE,)\n",
    "    assert y.shape == (BATCH_SIZE, len(CLASSES.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model(learning_rate):\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        hub.KerasLayer(HUB, output_shape=[128], input_shape=[], dtype=tf.string),\n",
    "        Dense(500,activation='relu'),\n",
    "        Dense(100,activation='relu'),\n",
    "        Dense(len(CLASSES), activation='softmax'),    \n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_dnn_model(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "MODEL_DIR = \"./models/txtclf/dnn\"\n",
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=eval_dataset,\n",
    "    callbacks=[TensorBoard(MODEL_DIR, embeddings_freq=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We get 80% validation accuracy. Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Model with CNN\n",
    "\n",
    "While the above method shines in simplicity, it uses a sentence level embedding which ignores the ordering of words. Might we get better performance if we embedded each word individually then fed them into a sequential model? We test that hypothesis now.\n",
    "\n",
    "The `hub.KerasLayer()` method doesn't support word level embeddings natively, instead it averages the component word embeddings into a single sentence embedding, so to achieve what we want we must do it upfront in the `input_fn()`. In particular we:\n",
    "1. Split each sentence into a list of its component words\n",
    "2. Pad each list to a constant length\n",
    "3. Embed each word into 128 dimension vector representation\n",
    "\n",
    "Note the changes to the `input_fn()` below.\n",
    "\n",
    "Since input function now returns a sequence of word embeddings, so we can process the data using a sequential model. Specifically we'll use a 1D CNN. Note the changes to `keras_model()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts, labels, batch_size, mode):\n",
    "    labels = tf.one_hot(labels, len(CLASSES))\n",
    "    texts = [sentence.split() for sentence in texts]\n",
    "    texts = [\n",
    "        (sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH]\n",
    "        for sentence in texts]\n",
    "    embed = hub.load(HUB)\n",
    "    texts = [embed(sentence) for sentence in texts]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else:\n",
    "        return dataset.shuffle(50000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The subsequent cell takes ~ 3 hours on CPU, about ~ 6 minutes on a P100 GPU, and ~ 4 minutes on a V100 GPU**\n",
    "\n",
    "This takes so long because now we are doing a lot of pre-processing in the input function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dataset = create_dataset(X_train, Y_train, BATCH_SIZE, mode=TRAIN)\n",
    "\n",
    "eval_dataset = create_dataset(X_test, Y_test, BATCH_SIZE, mode=EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cell\n",
    "EMBEDDING_DIM = 128\n",
    "for x, y in train_dataset.take(1):\n",
    "    assert x.shape == (BATCH_SIZE, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    assert y.shape == (BATCH_SIZE, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dropout(\n",
    "            input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "            rate=dropout_rate\n",
    "        ),\n",
    "        Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            bias_initializer='random_uniform',\n",
    "            padding='same',\n",
    "        ),\n",
    "        MaxPooling1D(pool_size=pool_size),\n",
    "        Conv1D(\n",
    "            filters=filters * 2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            bias_initializer='random_uniform',\n",
    "            padding='same',\n",
    "        ),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        Dense(N_CLASSES, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn_model(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy improved to 83%! Looks like paying attention to word order does help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
