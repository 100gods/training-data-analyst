{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Keras Functional API\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand embeddings and how to create them with the feature column API\n",
    "  - Understand Deep and Wide models and when to use them\n",
    "  - Understand the Keras functional API and how to build a deep and wide model with it\n",
    "  - Learn how to train a Keras model at scale on GCP\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the last notebook, we learned about the Keras Sequential API. The [Keras Functional API](https://www.tensorflow.org/guide/keras#functional_api) provides an alternate way of building models which is more flexible. With the Functional API, we can build models with more complex topologies, multiple input or output layers, shared layers or non-sequential data flows (e.g. residual layers).\n",
    "\n",
    "In this notebook we'll use what we learned about feature columns to build a Wide & Deep model. Recall, that the idea behind Wide & Deep models is to join the two methods of learning through memorization and generalization by making a wide linear model and a deep learning model to accommodate both. \n",
    "\n",
    "<img src='assets/wide_deep.png' width='80%'>\n",
    "<sup>(image: https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)</sup>\n",
    "\n",
    "The Wide part of the model is associated with the memory element. In this case, we train a linear model with a wide set of crossed features and learn the correlation of this related data with the assigned label. The Deep part of the model is associated with the generalization element where we use embedding vectors for features. The best embeddings are then learned through the training process. While both of these methods can work well alone, Wide & Deep models excel by combining these techniques together. \n",
    "\n",
    "Once we have trained our model, we will see how to train our model at scale on GCP using AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-nightly-2.0-preview==2.0.0.dev20190919\r\n"
     ]
    }
   ],
   "source": [
    "#  Ensure that we have the latest version of Tensorflow installed.\n",
    "!pip3 freeze | grep tf-nightly-2.0-preview || pip3 install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190919\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data \n",
    "\n",
    "We will use the taxifare dataset, using the CSV files that we created in the first notebook of this sequence. Those files have been saved into `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 munn  primarygroup  123590 Sep 19 18:08 ../data/taxi-test.csv\r\n",
      "-rw-r--r--  1 munn  primarygroup  579055 Sep 19 18:08 ../data/taxi-train.csv\r\n",
      "-rw-r--r--  1 munn  primarygroup  123114 Sep 19 18:08 ../data/taxi-valid.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these functions for reading data from the csv files above in the [previous notebook](2_dataset_api.ipynb). For this lab we will also include some additional engineered features in our model. In particular, we will compute the difference in latitude and longitude, as well as the Euclidean distance between the pick-up and drop-off locations. We can accomplish this by adding these new features to the features dictionary with the function `add_engineered_features` below. \n",
    "\n",
    "Note that we include a call to this function when collecting our features dict and labels in the `features_and_labels` function below as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "def add_engineered_features(features):\n",
    "    # Compute Euclidean distance\n",
    "    features[\"latdiff\"] = features[\"pickup_latitude\"] - features[\"dropoff_latitude\"]\n",
    "    features[\"londiff\"] = features[\"pickup_longitude\"] - features[\"dropoff_longitude\"]\n",
    "    features[\"euclidean_dist\"] = tf.sqrt(\n",
    "        x=features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    \n",
    "    # Add engineered features\n",
    "    features = add_engineered_features(features)\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature columns for Wide and Deep model\n",
    "\n",
    "For the Wide columns, we will create feature columns of crossed features. To do this, we'll create a collection of Tensorflow feature columns to pass to the `tf.feature_column.crossed_column` constructor. The Deep columns will consist of numeric columns and the embedding columns we want to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(start=38.0, stop=42.0, num=NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start=-76.0, stop=-72.0, num=NBUCKETS).tolist()\n",
    "\n",
    "fc_bucketized_plat = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(key=\"pickup_longitude\"), boundaries=lonbuckets)\n",
    "fc_bucketized_plon = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(key=\"pickup_latitude\"), boundaries=latbuckets)\n",
    "fc_bucketized_dlat = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(key=\"dropoff_longitude\"), boundaries=lonbuckets)\n",
    "fc_bucketized_dlon = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(key=\"dropoff_latitude\"), boundaries=latbuckets)\n",
    "\n",
    "# 2. Cross features for locations\n",
    "fc_crossed_dloc = fc.crossed_column(\n",
    "    keys=[fc_bucketized_dlat, fc_bucketized_dlon],\n",
    "    hash_bucket_size=NBUCKETS * NBUCKETS)\n",
    "fc_crossed_ploc = fc.crossed_column(\n",
    "    keys=[fc_bucketized_plat, fc_bucketized_plon],\n",
    "    hash_bucket_size=NBUCKETS * NBUCKETS)\n",
    "fc_crossed_pd_pair = fc.crossed_column(\n",
    "    keys=[fc_crossed_dloc, fc_crossed_ploc],\n",
    "    hash_bucket_size=NBUCKETS**4)\n",
    "\n",
    "# 3. Create embedding columns for the crossed columns\n",
    "fc_pd_pair = fc.embedding_column(categorical_column=fc_crossed_pd_pair, dimension=3)\n",
    "fc_dloc = fc.embedding_column(categorical_column=fc_crossed_dloc, dimension=3)\n",
    "fc_ploc = fc.embedding_column(categorical_column=fc_crossed_ploc, dimension=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather list of feature columns\n",
    "\n",
    "Next we gather the list of wide and deep feature columns we'll pass to our Wide & Deep model in Tensorflow. Recall, wide columns are sparse, have linear relationship with the output while continuous columns are deep, have a complex relationship with the output. We will use our previously bucketized columns to collect crossed feature columns and sparse feature columns for our wide columns, and embedding feature columns and numeric features columns for the deep columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns = [\n",
    "    # One-hot encoded feature crosses\n",
    "    fc.indicator_column(fc_crossed_dloc),\n",
    "    fc.indicator_column(fc_crossed_ploc),\n",
    "    fc.indicator_column(fc_crossed_pd_pair)\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    # Embedding_column to \"group\" together ...\n",
    "    fc.embedding_column(categorical_column=fc_crossed_pd_pair, dimension=10),\n",
    "\n",
    "    # Numeric columns\n",
    "    fc.numeric_column(key=\"pickup_latitude\"),\n",
    "    fc.numeric_column(key=\"pickup_longitude\"),\n",
    "    fc.numeric_column(key=\"dropoff_longitude\"),\n",
    "    fc.numeric_column(key=\"dropoff_latitude\"),\n",
    "    fc.numeric_column(key=\"latdiff\"),\n",
    "    fc.numeric_column(key=\"londiff\"),\n",
    "    fc.numeric_column(key=\"euclidean_dist\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Wide and Deep model in Keras\n",
    "\n",
    "To build a wide-and-deep network, we connect the sparse (i.e. wide) features directly to the output node, but pass the dense (i.e. deep) features through a set of fully connected layers. Here’s that model architecture looks using the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLS = [\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'latdiff',\n",
    "    'londiff',\n",
    "    'euclidean_dist'\n",
    "]\n",
    "\n",
    "inputs = {colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "          for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pickup_longitude': <tf.Tensor 'pickup_longitude:0' shape=(None,) dtype=float32>,\n",
       " 'pickup_latitude': <tf.Tensor 'pickup_latitude:0' shape=(None,) dtype=float32>,\n",
       " 'dropoff_longitude': <tf.Tensor 'dropoff_longitude:0' shape=(None,) dtype=float32>,\n",
       " 'dropoff_latitude': <tf.Tensor 'dropoff_latitude:0' shape=(None,) dtype=float32>,\n",
       " 'passenger_count': <tf.Tensor 'passenger_count:0' shape=(None,) dtype=float32>,\n",
       " 'latdiff': <tf.Tensor 'latdiff:0' shape=(None,) dtype=float32>,\n",
       " 'londiff': <tf.Tensor 'londiff:0' shape=(None,) dtype=float32>,\n",
       " 'euclidean_dist': <tf.Tensor 'euclidean_dist:0' shape=(None,) dtype=float32>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3089: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:353: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/ops/embedding_ops.py:810: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4265: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the deep part of model\n",
    "deep = tf.keras.layers.DenseFeatures(deep_columns, name='deep_inputs')(inputs)\n",
    "\n",
    "dnn_hidden_units = [10,5]\n",
    "for numnodes in dnn_hidden_units:\n",
    "    deep = tf.keras.layers.Dense(numnodes, activation='relu')(deep) \n",
    "\n",
    "# Create the wide part of model\n",
    "wide = tf.keras.layers.DenseFeatures(wide_columns, name='wide_inputs')(inputs)\n",
    "\n",
    "# Combine deep and wide parts of the model\n",
    "combined = tf.keras.layers.concatenate(inputs=[deep, wide], name='combined')\n",
    "\n",
    "# Map the combined outputs into a single prediction value\n",
    "output = tf.keras.layers.Dense(units=1, activation=None, name='prediction')(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the model\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\"dot\" not found in path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1879\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2ce4f52460b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    281\u001b[0m                      \u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrankdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                      \u001b[0mexpand_nested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_nested\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                      dpi=dpi)\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'IPython.core.magics.namespace'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;31m# We don't raise an exception here in order to avoid crashing notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mcheck_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Attempt to create an image of a blank graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1881\u001b[0m                 raise Exception(\n\u001b[1;32m   1882\u001b[0m                     '\"{prog}\" not found in path.'.format(\n\u001b[0;32m-> 1883\u001b[0;31m                         prog=prog))\n\u001b[0m\u001b[1;32m   1884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: \"dot\" not found in path."
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:540: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /Users/munn/Projects/wBenoit/asl-dev/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:214: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 50  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern='../data/taxi-train*',\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern='../data/taxi-valid*',\n",
    "    batch_size=1000,\n",
    "    mode=tf.estimator.ModeKeys.EVAL).take(NUM_EVAL_EXAMPLES//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom evalution metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1 steps, validate for 10 steps\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 15s 15s/step - loss: 220.3724 - rmse: 14.8449 - mse: 220.3724 - val_loss: 247.9802 - val_rmse: 15.7295 - val_mse: 247.9803\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 242.5711 - rmse: 15.5747 - mse: 242.5711 - val_loss: 247.8867 - val_rmse: 15.7266 - val_mse: 247.8867\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 227.5333 - rmse: 15.0842 - mse: 227.5333 - val_loss: 247.7934 - val_rmse: 15.7236 - val_mse: 247.7934\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 266.0244 - rmse: 16.3103 - mse: 266.0244 - val_loss: 247.6998 - val_rmse: 15.7206 - val_mse: 247.6998\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 224.9883 - rmse: 14.9996 - mse: 224.9883 - val_loss: 247.6064 - val_rmse: 15.7176 - val_mse: 247.6064\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 246.0034 - rmse: 15.6845 - mse: 246.0034 - val_loss: 247.5131 - val_rmse: 15.7147 - val_mse: 247.5131\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 219.3127 - rmse: 14.8092 - mse: 219.3127 - val_loss: 247.4199 - val_rmse: 15.7117 - val_mse: 247.4198\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 248.1050 - rmse: 15.7513 - mse: 248.1050 - val_loss: 247.3266 - val_rmse: 15.7087 - val_mse: 247.3266\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 206.4869 - rmse: 14.3697 - mse: 206.4869 - val_loss: 247.2338 - val_rmse: 15.7058 - val_mse: 247.2339\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 274.7976 - rmse: 16.5770 - mse: 274.7976 - val_loss: 247.1409 - val_rmse: 15.7028 - val_mse: 247.1409\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 221.2585 - rmse: 14.8748 - mse: 221.2585 - val_loss: 247.0482 - val_rmse: 15.6998 - val_mse: 247.0482\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 219.3471 - rmse: 14.8104 - mse: 219.3471 - val_loss: 246.9555 - val_rmse: 15.6969 - val_mse: 246.9555\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 218.4044 - rmse: 14.7785 - mse: 218.4044 - val_loss: 246.8630 - val_rmse: 15.6939 - val_mse: 246.8630\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 224.4406 - rmse: 14.9813 - mse: 224.4406 - val_loss: 246.7703 - val_rmse: 15.6910 - val_mse: 246.7702\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 232.7795 - rmse: 15.2571 - mse: 232.7795 - val_loss: 246.6774 - val_rmse: 15.6880 - val_mse: 246.6774\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 213.7296 - rmse: 14.6195 - mse: 213.7296 - val_loss: 246.5848 - val_rmse: 15.6851 - val_mse: 246.5848\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 250.7330 - rmse: 15.8346 - mse: 250.7330 - val_loss: 246.4919 - val_rmse: 15.6821 - val_mse: 246.4919\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 222.0775 - rmse: 14.9023 - mse: 222.0775 - val_loss: 246.3992 - val_rmse: 15.6791 - val_mse: 246.3992\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 193.5035 - rmse: 13.9106 - mse: 193.5035 - val_loss: 246.3069 - val_rmse: 15.6762 - val_mse: 246.3069\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 206.7524 - rmse: 14.3789 - mse: 206.7524 - val_loss: 246.2147 - val_rmse: 15.6733 - val_mse: 246.2147\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 205.0543 - rmse: 14.3197 - mse: 205.0543 - val_loss: 246.1228 - val_rmse: 15.6703 - val_mse: 246.1228\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 258.1689 - rmse: 16.0676 - mse: 258.1689 - val_loss: 246.0305 - val_rmse: 15.6674 - val_mse: 246.0305\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 235.8821 - rmse: 15.3585 - mse: 235.8821 - val_loss: 245.9383 - val_rmse: 15.6644 - val_mse: 245.9382\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 209.8706 - rmse: 14.4869 - mse: 209.8706 - val_loss: 245.8460 - val_rmse: 15.6615 - val_mse: 245.8460\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 202.0488 - rmse: 14.2144 - mse: 202.0488 - val_loss: 245.7540 - val_rmse: 15.6585 - val_mse: 245.7540\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 217.4776 - rmse: 14.7471 - mse: 217.4776 - val_loss: 245.6620 - val_rmse: 15.6556 - val_mse: 245.6620\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 222.9679 - rmse: 14.9321 - mse: 222.9679 - val_loss: 245.5700 - val_rmse: 15.6526 - val_mse: 245.5700\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 216.7222 - rmse: 14.7215 - mse: 216.7222 - val_loss: 245.4780 - val_rmse: 15.6497 - val_mse: 245.4780\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 225.1905 - rmse: 15.0063 - mse: 225.1905 - val_loss: 245.3862 - val_rmse: 15.6468 - val_mse: 245.3862\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 235.1756 - rmse: 15.3354 - mse: 235.1756 - val_loss: 245.2942 - val_rmse: 15.6438 - val_mse: 245.2942\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 258.0954 - rmse: 16.0653 - mse: 258.0954 - val_loss: 245.2022 - val_rmse: 15.6409 - val_mse: 245.2022\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 219.5739 - rmse: 14.8180 - mse: 219.5739 - val_loss: 245.1102 - val_rmse: 15.6379 - val_mse: 245.1102\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 247.3748 - rmse: 15.7282 - mse: 247.3748 - val_loss: 245.0182 - val_rmse: 15.6350 - val_mse: 245.0182\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 230.3140 - rmse: 15.1761 - mse: 230.3140 - val_loss: 244.9264 - val_rmse: 15.6321 - val_mse: 244.9265\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 231.0120 - rmse: 15.1991 - mse: 231.0120 - val_loss: 244.8346 - val_rmse: 15.6291 - val_mse: 244.8346\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 208.1112 - rmse: 14.4261 - mse: 208.1112 - val_loss: 244.7430 - val_rmse: 15.6262 - val_mse: 244.7430\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 189.5846 - rmse: 13.7690 - mse: 189.5846 - val_loss: 244.6516 - val_rmse: 15.6232 - val_mse: 244.6516\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 201.0168 - rmse: 14.1780 - mse: 201.0168 - val_loss: 244.5605 - val_rmse: 15.6203 - val_mse: 244.5605\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 215.9475 - rmse: 14.6952 - mse: 215.9475 - val_loss: 244.4692 - val_rmse: 15.6174 - val_mse: 244.4693\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 206.1094 - rmse: 14.3565 - mse: 206.1094 - val_loss: 244.3780 - val_rmse: 15.6145 - val_mse: 244.3780\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 205.9430 - rmse: 14.3507 - mse: 205.9430 - val_loss: 244.2869 - val_rmse: 15.6116 - val_mse: 244.2869\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 249.4964 - rmse: 15.7955 - mse: 249.4964 - val_loss: 244.1957 - val_rmse: 15.6086 - val_mse: 244.1957\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 230.6804 - rmse: 15.1882 - mse: 230.6804 - val_loss: 244.1045 - val_rmse: 15.6057 - val_mse: 244.1045\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 221.1992 - rmse: 14.8728 - mse: 221.1992 - val_loss: 244.0131 - val_rmse: 15.6028 - val_mse: 244.0131\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 207.0673 - rmse: 14.3898 - mse: 207.0673 - val_loss: 243.9220 - val_rmse: 15.5999 - val_mse: 243.9220\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 218.7977 - rmse: 14.7918 - mse: 218.7977 - val_loss: 243.8309 - val_rmse: 15.5969 - val_mse: 243.8309\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 200.7046 - rmse: 14.1670 - mse: 200.7046 - val_loss: 243.7401 - val_rmse: 15.5940 - val_mse: 243.7401\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 211.5052 - rmse: 14.5432 - mse: 211.5052 - val_loss: 243.6493 - val_rmse: 15.5911 - val_mse: 243.6493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 219.9075 - rmse: 14.8293 - mse: 219.9075 - val_loss: 243.5587 - val_rmse: 15.5882 - val_mse: 243.5587\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 197.3156 - rmse: 14.0469 - mse: 197.3156 - val_loss: 243.4683 - val_rmse: 15.5853 - val_mse: 243.4683\n",
      "CPU times: user 8min 2s, sys: 7min 42s, total: 15min 45s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "OUTDIR = \"./taxi_trained\"\n",
    "shutil.rmtree(path=OUTDIR, ignore_errors=True) # start fresh each time\n",
    "history = model.fit(x=trainds,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=NUM_EVALS,\n",
    "                    validation_data=evalds,\n",
    "                    callbacks=[TensorBoard(OUTDIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
