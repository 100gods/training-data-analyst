{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- add attention (tf.keras.layers.Attention)\n",
    "- see if benchmarks improved\n",
    "- deploy for online prediction\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "- Attention Keras: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "- TF NMT: https://colab.sandbox.google.com/drive/1R4Hxvzf1a6H95N2sjh5_lVRat_59Zxlx#scrollTo=ddefjBMa3jF0\n",
    "\n",
    "### To research\n",
    "\n",
    "- Better understanding of teacher forcing: https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n",
    "\n",
    "#### completed\n",
    "\n",
    "- make reproducible\n",
    "- prediction works\n",
    "- establish benchmark on: bleu score (do after the fact, then maybe add as keras eval metric)\n",
    "- add way to checkpoint/restore models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "print(tf.__version__) # 2.0.0-beta0\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "LOAD_CHECKPOINT=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t多Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. lower case\n",
    "2. add space between puncation and words\n",
    "3. replace tokens that aren't a-z or punctation with space\n",
    "4. add \\<start> and \\<end> tokens\n",
    "5. tokenize \n",
    "6. pad to length of longest sentence (post-pad)\n",
    "7. convert to tf.data dataset\n",
    "8. shuffle and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit size to 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang,tensor):\n",
    "    return [lang.index_word[t] if t!=0 else '' for t in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "[  1 133  14 316   3   2   0   0   0   0   0   0   0   0   0   0]\n",
      "['<start>', 'deja', 'de', 'leer', '.', '<end>', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Target Language; index to word mapping\n",
      "[  1  86 341   3   2   0   0   0   0   0   0]\n",
      "['<start>', 'stop', 'reading', '.', '<end>', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "print(input_tensor_train[0])\n",
    "print(convert(inp_lang, input_tensor_train[0]))\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "print(target_tensor_train[0])\n",
    "print(convert(targ_lang, target_tensor_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0616 02:36:59.587545 140501408745216 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def encoder_decoder_dataset(encoder_input, decoder_input):\n",
    "    \"\"\"Converts sequence pairs into a tf.data.Dataset suitable for\n",
    "    encoder-decoder learnign using teacher forcing.\n",
    "    \n",
    "    Arguments:\n",
    "        encoder_input: tensor of shape (num_examples,encoder_ seq_length) fed into the encoder RNN\n",
    "        decoder_input: tensor of shape (num_examples,decoder_seq_length) fed into the decoder RNN during training\n",
    "    Returns\n",
    "        tf.data.Dataset of shape ((encoder_input,decoder_input),target)\n",
    "        target is the decoder_input shifted ahead by 1, with a 0 for padding at the end\n",
    "    \"\"\"\n",
    "    target = tf.roll(decoder_input,-1,1) # shift ahead by 1\n",
    "    target = tf.concat((target[:,:-1],tf.zeros([target.shape[0],1],dtype=tf.int32)),axis=-1) # replace last column with 0s\n",
    "    return tf.data.Dataset.from_tensor_slices(((encoder_input, decoder_input), target))\n",
    "    \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "train_dataset = encoder_decoder_dataset(input_tensor_train, target_tensor_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "eval_dataset = encoder_decoder_dataset(input_tensor_val, target_tensor_val).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=65, shape=(3, 16), dtype=int32, numpy=\n",
       " array([[   1,    4, 5125,    3,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 3281,    3,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 1182,   32,   22,   50,    3,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: id=69, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   1,    5, 1953,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   28,   38,  230,    3,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,  244,  128,   49,   56,    3,    2,    0,    0,    0,    0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: id=73, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   5, 1953,    3,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  28,   38,  230,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [ 244,  128,   49,   56,    3,    2,    0,    0,    0,    0,    0]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_encoder_input_batch, example_decoder_input_batch), example_target_batch = next(iter(train_dataset))\n",
    "example_encoder_input_batch[:3], example_decoder_input_batch[:3], example_target_batch[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 244 ms, total: 4.58 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"encoder_input\")\n",
    "encoder_inputs_embedded = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim,input_length=max_length_inp)(encoder_inputs)\n",
    "encoder_outputs, encoder_state = tf.keras.layers.GRU(\n",
    "     units = 1024,\n",
    "     return_sequences=True,\n",
    "     return_state=True, # what is recurrent_initializer?\n",
    "     recurrent_initializer='glorot_uniform')(encoder_inputs_embedded)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"decoder_input\")\n",
    "decoder_inputs_embedded = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim,input_length=max_length_targ)(decoder_inputs)\n",
    "decoder_rnn = tf.keras.layers.GRU(\n",
    "    units = 1024,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer='glorot_uniform')\n",
    "decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded,initial_state=encoder_state)\n",
    "\n",
    "# Classifier (take each intermediate hidden state and predict word)\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_tar_size, activation='softmax')\n",
    "predictions = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model definition\n",
    "if LOAD_CHECKPOINT:\n",
    "    model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'model.h5'))\n",
    "else:\n",
    "    model = tf.keras.models.Model(inputs=[encoder_inputs,decoder_inputs], outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') # metrics=[bleu_1]\n",
    "    model.summary()\n",
    "    model.fit(train_dataset,validation_data=eval_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We can't just use model.predict(), because we don't know all the inputs we used during training. We only know the encoder_input (source language) but not the decoder_input (target language).\n",
    "\n",
    "We do however know the first token of the decoder input, which is the START character. So using this plus the state of the encoder RNN, we can predict the next token. We will then use that token to be the second token of decoder input, and continue like this until we predict the END token, or we reach some defined max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 02:37:04.155453 140501408745216 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0616 02:37:04.357814 140501408745216 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    encoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "    decoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'decoder_model.h5'))\n",
    "    \n",
    "else:\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "    decoder_state_input = tf.keras.layers.Input(shape=(units,),name=\"decoder_state_input\")\n",
    "\n",
    "    decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded, initial_state=decoder_state_input) # reuses layer weights\n",
    "    predictions = decoder_dense(decoder_outputs) # reuses layer weights\n",
    "\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs,decoder_state_input],\n",
    "        [predictions,decoder_state])\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = tf.constant([[1]])\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    for _ in range(max_length_targ):\n",
    "        output_tokens, decoder_state = decoder_model.predict(\n",
    "            [target_seq,states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = targ_lang.index_word[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == '<end>': break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = tf.constant([[sampled_token_index]])\n",
    "\n",
    "        # Update states\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input:\n",
      "['quise', 'pagar', '.', '<end>', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['i', 'wanted', 'to', 'pay', '.', '<end>', '', '', '', '']\n",
      "Machine Translation:\n",
      "['i', 'wanted', 'them', 'to', 'know', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['yo', 'camino', 'todos', 'los', 'dias', '.', '<end>', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['i', 'walk', 'every', 'day', '.', '<end>', '', '', '', '']\n",
      "Machine Translation:\n",
      "['i', 'swim', 'every', 'day', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['tom', 'no', 'comio', 'nada', '.', '<end>', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['tom', 'ate', 'nothing', '.', '<end>', '', '', '', '', '']\n",
      "Machine Translation:\n",
      "['tom', 'didn', 't', 'eat', 'anything', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['su', 'respuesta', 'es', 'erronea', '.', '<end>', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['your', 'answer', 'is', 'wrong', '.', '<end>', '', '', '', '']\n",
      "Machine Translation:\n",
      "['her', 'answer', 'is', 'funny', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['te', 'veo', 'por', 'ahi', '.', '<end>', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['see', 'you', 'around', '.', '<end>', '', '', '', '', '']\n",
      "Machine Translation:\n",
      "['see', 'you', 'guys', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['ahora', 'depende', 'de', 'mi', '.', '<end>', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['now', 'it', 's', 'up', 'to', 'me', '.', '<end>', '', '']\n",
      "Machine Translation:\n",
      "['now', 'it', 's', 'up', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['pasenla', 'bien', '.', '<end>', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['have', 'fun', '.', '<end>', '', '', '', '', '', '']\n",
      "Machine Translation:\n",
      "['enjoy', 'yourself', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['ven', 'enseguida', '.', '<end>', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['come', 'at', 'once', '.', '<end>', '', '', '', '', '']\n",
      "Machine Translation:\n",
      "['come', 'immediately', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['tom', 'esta', 'grogui', '.', '<end>', '', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['tom', 's', 'groggy', '.', '<end>', '', '', '', '', '']\n",
      "Machine Translation:\n",
      "['tom', 's', 'stoned', '.', '<end>']\n",
      "-\n",
      "Input:\n",
      "['soy', 'todo', 'oidos', '.', '<end>', '', '', '', '', '', '', '', '', '', '']\n",
      "Reference Translation:\n",
      "['i', 'm', 'all', 'ears', '.', '<end>', '', '', '', '']\n",
      "Machine Translation:\n",
      "['i', 'am', 'all', 'ears', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    decoded_sentence = decode_sequence(input_tensor_val[seq_index: seq_index + 1])\n",
    "    print('-')\n",
    "    print('Input:')\n",
    "    print(convert(inp_lang, input_tensor_val[seq_index][1:]))\n",
    "    print('Reference Translation:')\n",
    "    print(convert(targ_lang, target_tensor_val[seq_index][1:]))\n",
    "    print('Machine Translation:')\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "model.save(os.path.join(MODEL_PATH,'model.h5'))\n",
    "encoder_model.save(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "decoder_model.save(os.path.join(MODEL_PATH,'decoder_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Our loss metric, cross entropy log loss, is not the best eval metric for machine translation.\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation.\n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLUE).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "It has the advantages that it allows comparison to multiple reference translations, and allows flexibility for the ordering of words and phrases. It still is imperfect, since it gives no credit to synonyms, so human evaluation is still best\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match. In practice on about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "The NLTK framework has an implementation that we will use.\n",
    "\n",
    "Furthermore we can't run evaluation during training, because at that time the correct decoder input is used. We will run our eval metrics after.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_1(reference, candidate):\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (1,),smoothing_function)\n",
    "\n",
    "def bleu_4(reference, candidate):\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (.25,.25,.25,.25),smoothing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes ~ 3min to run, the bulk of which is decoding the 6000 sentences in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_examples = len(input_tensor_val)\n",
    "bleu_1_total = 0\n",
    "bleu_4_total = 0\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    reference_sentence = convert(targ_lang, target_tensor_val[idx][1:])\n",
    "    decoded_sentence = decode_sequence(input_tensor_val[idx:idx+1])\n",
    "    bleu_1_total += bleu_1(reference_sentence,decoded_sentence)\n",
    "    bleu_4_total += bleu_4(reference_sentence,decoded_sentence)\n",
    "print('BLEU 1: {}'.format(bleu_1_total/num_examples))\n",
    "print('BLEU 4: {}'.format(bleu_4_total/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "- Batch_Size: 64\n",
    "- Optimizer: adam\n",
    "- Embed_dim: 256\n",
    "- GRU Units: 1024\n",
    "- Train Examples: 24,000\n",
    "- Epochs: 10\n",
    "- Hardware: P100 GPU\n",
    "\n",
    "**Baseline**\n",
    "- 5min - loss: 0.0722 - val_loss: 0.9062\n",
    "- BLEU 1: 0.2519574312515255\n",
    "- BLEU 4: 0.04589972764144636\n",
    "- Manuel Inspection:most translations make sense, but are synonyms not exact matches\n",
    "\n",
    "Expect loss to be considerably higher once targets are no longer the same as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Note that to decode our sequences we're not just calling .predict() on a single Keras model. We're using .predict() on two different models with some python code in between. On top of that we're calling each model multiple times in a for loop.\n",
    "\n",
    "Because of this we can't just export to SavedModel and deploy to AI Platform. Instead we'll take advantage of AI Platforms custom prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
