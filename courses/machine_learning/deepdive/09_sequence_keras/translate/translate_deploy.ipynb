{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF 2.0 Dependency\n",
    "\n",
    "This code relies on TF 2.0 which AI Platform online prediction doesn't support yet. Prediction works locally, and the model deploys, but online prediction fails because the AI Platform nodes are running TF 1.13.\n",
    "\n",
    "**We need to re-test this notebook once TF 2.0 is supported for online prediction.** With any luck it will work without any further changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'vijays-sandbox'\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "MODEL_NAME = 'translate'\n",
    "VERSION_NAME = 'v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy for Online Prediction\n",
    "\n",
    "To get our translations we're not just calling .predict() on a single Keras model. We're using .predict() on two different models with some python code in between. On top of that we're calling each model multiple times in a for loop.\n",
    "\n",
    "Because of this we can't just export to SavedModel and deploy to AI Platform. \n",
    "\n",
    "Instead we'll take advantage of [AI Platforms Custom Prediction Routines](https://cloud.google.com/ml-engine/docs/tensorflow/custom-prediction-routines) which allows us to execute custom python code in response to every online prediction request. There are 5 steps to creating a custom prediction routine:\n",
    "\n",
    "1. Upload Model Artifacts to GCS\n",
    "2. Implement Predictor interface \n",
    "3. Package the prediction code and dependencies\n",
    "4. Deploy\n",
    "5. Invoke API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Model Artifacts to GCS\n",
    "\n",
    "Here we upload our model weights (encoder_model.h5 and decoder_model.h5) and our tokenizer objects so that AI Platform can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://translate_models/baseline/encoder_model.h5 [Content-Type=application/octet-stream]...\n",
      "Copying file://translate_models/baseline/decoder_model.h5 [Content-Type=application/octet-stream]...\n",
      "\\\n",
      "Operation completed over 2 objects/63.4 MiB.                                     \n",
      "Copying file://translate_models/baseline/encoder_tokenizer.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://translate_models/baseline/decoder_tokenizer.pkl [Content-Type=application/octet-stream]...\n",
      "/ [2 files][652.2 KiB/652.2 KiB]                                                \n",
      "Operation completed over 2 objects/652.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $MODEL_PATH/encoder_model.h5 $MODEL_PATH/decoder_model.h5 gs://$BUCKET/translate/model/\n",
    "!gsutil cp $MODEL_PATH/encoder_tokenizer.pkl $MODEL_PATH/decoder_tokenizer.pkl gs://$BUCKET/translate/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Predictor Interface\n",
    "\n",
    "Interface Spec: https://cloud.google.com/ml-engine/docs/tensorflow/custom-prediction-routines#predictor-class\n",
    "\n",
    "This tells AI Platform how to load the model artifacts, and is where we specify our custom prediction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor.py\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils_preproc\n",
    "\n",
    "MAX_TRANSLATE_LENGTH = 11 \n",
    "\n",
    "class TranslatePredictor(object):\n",
    "    def __init__(self, encoder_model, encoder_tokenizer, \n",
    "                 decoder_model, decoder_tokenizer):\n",
    "      self.encoder_model = encoder_model\n",
    "      self.encoder_tokenizer = encoder_tokenizer\n",
    "      self.decoder_model = decoder_model\n",
    "      self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "    \n",
    "    def _decode_sequences(self, input_seqs, output_tokenizer, max_decode_length=50):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        input_seqs: int tensor of shape (BATCH_SIZE,SEQ_LEN)\n",
    "        output_tokenizer: keras_preprocessing.text.Tokenizer used to conver from int to words\n",
    "\n",
    "        Returns translated sentences\n",
    "        \"\"\"\n",
    "        # Encode the input as state vectors.\n",
    "        batch_size = input_seqs.shape[0]\n",
    "        states_value = self.encoder_model.predict(input_seqs)\n",
    "\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq = tf.ones([batch_size,1])\n",
    "\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        decoded_sentences = [[] for _ in range(batch_size)]\n",
    "        for i in range(max_decode_length):\n",
    "            output_tokens, decoder_state = self.decoder_model.predict(\n",
    "                [target_seq,states_value])\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=-1)\n",
    "            tokens = utils_preproc.int2word(output_tokenizer,sampled_token_index)\n",
    "            for j in range (batch_size):\n",
    "                decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = tf.expand_dims(tf.constant(sampled_token_index),axis=-1)\n",
    "\n",
    "            # Update states\n",
    "            states_value = decoder_state\n",
    "\n",
    "        return decoded_sentences\n",
    "    \n",
    "    def predict(self, instances, **kwargs):\n",
    "        machine_translations = self._decode_sequences(\n",
    "            utils_preproc.preprocess(instances,self.encoder_tokenizer),\n",
    "            self.decoder_tokenizer,\n",
    "            MAX_TRANSLATE_LENGTH\n",
    "        )\n",
    "        return machine_translations\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        encoder_model = tf.keras.models.load_model(os.path.join(model_dir,'encoder_model.h5'))\n",
    "        decoder_model = tf.keras.models.load_model(os.path.join(model_dir,'decoder_model.h5'))\n",
    "    \n",
    "        encoder_tokenizer = pickle.load(open(os.path.join(model_dir,'encoder_tokenizer.pkl'),'rb'))\n",
    "        decoder_tokenizer = pickle.load(open(os.path.join(model_dir,'decoder_tokenizer.pkl'),'rb'))\n",
    "\n",
    "        return cls(encoder_model, encoder_tokenizer, decoder_model, decoder_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predictor Class Works Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 18:32:32.843926 140375193675520 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0620 18:32:33.056538 140375193675520 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['we', 're', 'not', 'eating', '.', '<end>', '', '', '', '', ''],\n",
       " ['winter', 'is', 'on', 'the', 'grass', '.', '<end>', '', '', '', ''],\n",
       " ['winter', 'is', 'coming', 'on', '.', '<end>', '', '', '', '', ''],\n",
       " ['tom', 'didn', 't', 'eat', 'lunch', '.', '<end>', '', '', '', ''],\n",
       " ['her', 'car', 'turned', 'red', '.', '<end>', '', '', '', '', ''],\n",
       " ['her', 'answer', 'is', 'weak', '.', '<end>', '', '', '', '', ''],\n",
       " ['how', 'far', 'is', 'a', 'secret', '?', '<end>', '', '', '', '']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import predictor\n",
    "\n",
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\", \n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]\n",
    "\n",
    "predictor = predictor.TranslatePredictor.from_path(MODEL_PATH)\n",
    "predictor.predict(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Package Predictor Class and Dependencies\n",
    "\n",
    "We must package the predictor as a tar.gz source distribution package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='translate_custom_predict_code',\n",
    "    version='0.1',\n",
    "    scripts=['predictor.py','utils_preproc.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/translate_custom_predict_code-0.1.tar.gz gs://$BUCKET/translate/predict_code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy\n",
    "\n",
    "This is similar to how we deploy standard models to AI Platform, with a few extra command line arguments.\n",
    "\n",
    "*Warning: If you get a GCS access error, grant the 'Storage Object Viewer' role on the bucket that contains your artifacts to the service account being used.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --regions us-central1\n",
    "\n",
    "#Change --runtime-version to 2.0 when supported\n",
    "!gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://$BUCKET/translate/model/ \\\n",
    "  --package-uris gs://$BUCKET/translate/predict_code/translate_custom_predict_code-0.1.tar.gz \\\n",
    "  --prediction-class predictor.TranslatePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "instances = [\n",
    "    [\"El soldado actuó valientemente.\"], \n",
    "    [\"Su pierna mala le impidió ganar la carrera.\"]\n",
    "]\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': instances}\n",
    ").execute()\n",
    "\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "else:\n",
    "  print(response['predictions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
