{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this lab we'll build a translation model from Spanish to English using an encoder-decoder model architecture. We'll benchmark our results using the industry standard BLEU score then deploy for online predictio using an AI Platform custom prediction routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk # for BLEU score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils_preproc\n",
    "\n",
    "print(tf.__version__) # 2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "LOAD_CHECKPOINT=False # True if you've already trained and don't want to re-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. The dataset contains Spanish-English  translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The `preprocess_sentence()` method does the following:\n",
    "1. Converts sentence lower case\n",
    "2. Adds a space between puncation and words\n",
    "3. Replaces tokens that aren't a-z or punctation with space\n",
    "4. Adds `<start>` and `<end>` tokens\n",
    "\n",
    "The `tokenize()` method does the following:\n",
    "    \n",
    "1. Maps each word to an integer\n",
    "2. Pads to length of longest sentence \n",
    "\n",
    "Note where each is being used in the subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(utils_preproc.preprocess_sentence(en_sentence))\n",
    "print(utils_preproc.preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[utils_preproc.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = utils_preproc.tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = utils_preproc.tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit size to 30000\n",
    "\n",
    "Since we'll be training on a single GPU, we'll use only the first 30K examples. We'll split this data 80/20 into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; int to word mapping\n",
      "[  1 133  14 316   3   2   0   0   0   0   0   0   0   0   0   0]\n",
      "['<start>', 'deja', 'de', 'leer', '.', '<end>', '', '', '', '', '', '', '', '', '', ''] \n",
      "\n",
      "Target Language; int to word mapping\n",
      "[  1  86 341   3   2   0   0   0   0   0   0]\n",
      "['<start>', 'stop', 'reading', '.', '<end>', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; int to word mapping\")\n",
    "print(input_tensor_train[0])\n",
    "print(utils_preproc.int2word(inp_lang, input_tensor_train[0]),'\\n')\n",
    "print(\"Target Language; int to word mapping\")\n",
    "print(target_tensor_train[0])\n",
    "print(utils_preproc.int2word(targ_lang, target_tensor_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset\n",
    "\n",
    "Note how our labels are our reference translations shifted ahead by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 18:07:26.343856 140665555871488 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def encoder_decoder_dataset(encoder_input, decoder_input):\n",
    "    \"\"\"Converts sequence pairs into a tf.data.Dataset suitable for\n",
    "    encoder-decoder learnign using teacher forcing.\n",
    "    \n",
    "    Arguments:\n",
    "    encoder_input: tensor of shape (num_examples,encoder_ seq_length) fed into the encoder RNN\n",
    "    decoder_input: tensor of shape (num_examples,decoder_seq_length) fed into the decoder RNN during training\n",
    "   \n",
    "    Returns:\n",
    "    tf.data.Dataset of shape ((encoder_input,decoder_input),target)\n",
    "        target is the decoder_input shifted ahead by 1, with a 0 for padding at the end\n",
    "    \"\"\"\n",
    "    target = tf.roll(decoder_input,-1,1) # shift ahead by 1\n",
    "    target = tf.concat((target[:,:-1],tf.zeros([target.shape[0],1],dtype=tf.int32)),axis=-1) # replace last column with 0s\n",
    "    return tf.data.Dataset.from_tensor_slices(((encoder_input, decoder_input), target))\n",
    "    \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "train_dataset = encoder_decoder_dataset(input_tensor_train, target_tensor_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "eval_dataset = encoder_decoder_dataset(input_tensor_val, target_tensor_val).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=65, shape=(3, 16), dtype=int32, numpy=\n",
       " array([[   1,    4, 5125,    3,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 3281,    3,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 1182,   32,   22,   50,    3,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: id=69, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   1,    5, 1953,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   28,   38,  230,    3,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,  244,  128,   49,   56,    3,    2,    0,    0,    0,    0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: id=73, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   5, 1953,    3,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  28,   38,  230,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [ 244,  128,   49,   56,    3,    2,    0,    0,    0,    0,    0]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_encoder_input_batch, example_decoder_input_batch), example_target_batch = next(iter(train_dataset))\n",
    "example_encoder_input_batch[:3], example_decoder_input_batch[:3], example_target_batch[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We use an encoder-decoder decoder architecture, however we embed our words into a latent space prior to feeding them into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    2409984     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1263360     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, None, 1024), 3938304     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, None, 1024), 3938304     embedding_1[0][0]                \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4935)   5058375     gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 16,608,327\n",
      "Trainable params: 16,608,327\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 2.1255 - val_loss: 1.6809\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 1.4693 - val_loss: 1.3911\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 1.1228 - val_loss: 1.1897\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 0.8308 - val_loss: 1.0542\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 22s 59ms/step - loss: 0.5741 - val_loss: 0.9742\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 0.3718 - val_loss: 0.9301\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 21s 57ms/step - loss: 0.2361 - val_loss: 0.9175\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 0.1542 - val_loss: 0.8977\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 0.1037 - val_loss: 0.9028\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 22s 58ms/step - loss: 0.0713 - val_loss: 0.9014\n",
      "CPU times: user 6min 10s, sys: 20.1 s, total: 6min 31s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"encoder_input\")\n",
    "encoder_inputs_embedded = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim,input_length=max_length_inp)(encoder_inputs)\n",
    "encoder_rnn = tf.keras.layers.GRU(\n",
    "     units = 1024,\n",
    "     return_sequences=True,\n",
    "     return_state=True, # what is recurrent_initializer?\n",
    "     recurrent_initializer='glorot_uniform')\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"decoder_input\")\n",
    "decoder_inputs_embedded = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim,input_length=max_length_targ)(decoder_inputs)\n",
    "decoder_rnn = tf.keras.layers.GRU(\n",
    "    units = 1024,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer='glorot_uniform')\n",
    "decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded,initial_state=encoder_state)\n",
    "\n",
    "# Classifier (take each intermediate hidden state and predict word)\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_tar_size, activation='softmax')\n",
    "predictions = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model definition\n",
    "if LOAD_CHECKPOINT:\n",
    "    model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'model.h5'))\n",
    "else:\n",
    "    model = tf.keras.models.Model(inputs=[encoder_inputs,decoder_inputs], outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') \n",
    "    model.summary()\n",
    "    model.fit(train_dataset,validation_data=eval_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We can't just use model.predict(), because we don't know all the inputs we used during training. We only know the encoder_input (source language) but not the decoder_input (target language).\n",
    "\n",
    "We do however know the first token of the decoder input, which is the `<start>` token. So using this plus the state of the encoder RNN, we can predict the next token. We will then use that token to be the second token of decoder input, and continue like this until we predict the `<end>` token, or we reach some defined max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    encoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "    decoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'decoder_model.h5'))\n",
    "    \n",
    "else:\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "    decoder_state_input = tf.keras.layers.Input(shape=(units,),name=\"decoder_state_input\")\n",
    "\n",
    "    decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded, initial_state=decoder_state_input) # reuses layer weights\n",
    "    predictions = decoder_dense(decoder_outputs) # reuses layer weights\n",
    "\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs,decoder_state_input],\n",
    "        [predictions,decoder_state])\n",
    "\n",
    "def decode_sequences(input_seqs, output_tokenizer, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_seqs: int tensor of shape (BATCH_SIZE,SEQ_LEN)\n",
    "    output_tokenizer: keras_preprocessing.text.Tokenizer used to conver from int to words\n",
    "    \n",
    "    Returns translated sentences\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    batch_size = input_seqs.shape[0]\n",
    "    states_value = encoder_model.predict(input_seqs)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = tf.ones([batch_size,1])\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentences = [[] for _ in range(batch_size)]\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model.predict(\n",
    "            [target_seq,states_value])\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=-1)\n",
    "        tokens = utils_preproc.int2word(output_tokenizer,sampled_token_index)\n",
    "        for j in range (batch_size):\n",
    "            decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = tf.expand_dims(tf.constant(sampled_token_index),axis=-1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "INPUT:\n",
      "No estamos comiendo.\n",
      "REFERENCE TRANSLATION:\n",
      "We're not eating.\n",
      "MACHINE TRANSLATION:\n",
      "['we', 're', 'not', 'eating', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Está llegando el invierno.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'on', 'the', 'grass', '.', '<end>', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "El invierno se acerca.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'coming', 'on', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Tom no comio nada.\n",
      "REFERENCE TRANSLATION:\n",
      "Tom ate nothing.\n",
      "MACHINE TRANSLATION:\n",
      "['tom', 'didn', 't', 'eat', 'lunch', '.', '<end>', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Su pierna mala le impidió ganar la carrera.\n",
      "REFERENCE TRANSLATION:\n",
      "His bad leg prevented him from winning the race.\n",
      "MACHINE TRANSLATION:\n",
      "['her', 'car', 'turned', 'red', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Su respuesta es erronea.\n",
      "REFERENCE TRANSLATION:\n",
      "Your answer is wrong.\n",
      "MACHINE TRANSLATION:\n",
      "['her', 'answer', 'is', 'weak', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "¿Qué tal si damos un paseo después del almuerzo?\n",
      "REFERENCE TRANSLATION:\n",
      "How about going for a walk after lunch?\n",
      "MACHINE TRANSLATION:\n",
      "['how', 'far', 'is', 'a', 'secret', '?', '<end>', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\", \n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Tom ate nothing.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"Your answer is wrong.\",\n",
    "    \"How about going for a walk after lunch?\"\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(\n",
    "    utils_preproc.preprocess(sentences,inp_lang),\n",
    "    targ_lang,\n",
    "    max_length_targ\n",
    ")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print('-')\n",
    "    print('INPUT:')\n",
    "    print(sentences[i])\n",
    "    print('REFERENCE TRANSLATION:')\n",
    "    print(reference_translations[i])\n",
    "    print('MACHINE TRANSLATION:')\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Model\n",
    "\n",
    "Save model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_CHECKPOINT:\n",
    "    os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "    model.save(os.path.join(MODEL_PATH,'model.h5'))\n",
    "    encoder_model.save(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "    decoder_model.save(os.path.join(MODEL_PATH,'decoder_model.h5'))\n",
    "    pickle.dump(inp_lang,open(os.path.join(MODEL_PATH,'encoder_tokenizer.pkl'),'wb'))\n",
    "    pickle.dump(targ_lang,open(os.path.join(MODEL_PATH,'decoder_tokenizer.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation. \n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLEU).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It allows flexibility for the ordering of words and phrases.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "It still is imperfect, since it gives no credit to synonyms and so human evaluation is still best when feasible. However BLEU is commonly considered the best among bad options for an automated metric.\n",
    "\n",
    "The NLTK framework has an implementation that we will use.\n",
    "\n",
    "We can't run calculate BLEU during training, because at that time the correct decoder input is used. Instead we'll calculate it now.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_1(reference, candidate):\n",
    "    reference = list(filter(lambda x: x != '', reference)) # remove padding\n",
    "    candidate = list(filter(lambda x: x != '', candidate)) # remove padding\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (1,),smoothing_function)\n",
    "\n",
    "def bleu_4(reference, candidate):\n",
    "    reference = list(filter(lambda x: x != '', reference)) # remove padding\n",
    "    candidate = list(filter(lambda x: x != '', candidate)) # remove padding\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (.25,.25,.25,.25),smoothing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes ~ 5min to run, the bulk of which is decoding the 6000 sentences in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 1: 0.2555061026047909\n",
      "BLEU 4: 0.04662961766141444\n",
      "CPU times: user 6min 41s, sys: 56.5 s, total: 7min 37s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_examples = len(input_tensor_val)\n",
    "bleu_1_total = 0\n",
    "bleu_4_total = 0\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    reference_sentence = utils_preproc.int2word(targ_lang, target_tensor_val[idx][1:])\n",
    "    decoded_sentence = decode_sequences(input_tensor_val[idx:idx+1],targ_lang,max_length_targ)[0]\n",
    "    bleu_1_total += bleu_1(reference_sentence,decoded_sentence)\n",
    "    bleu_4_total += bleu_4(reference_sentence,decoded_sentence)\n",
    "print('BLEU 1: {}'.format(bleu_1_total/num_examples))\n",
    "print('BLEU 4: {}'.format(bleu_4_total/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "- Batch_Size: 64\n",
    "- Optimizer: adam\n",
    "- Embed_dim: 256\n",
    "- GRU Units: 1024\n",
    "- Train Examples: 24,000\n",
    "- Epochs: 10\n",
    "- Hardware: P100 GPU\n",
    "\n",
    "**Performance**\n",
    "- Training Time: 5min \n",
    "- Cross-entropy loss: train: 0.0722 - val: 0.9062\n",
    "- BLEU 1: 0.2519574312515255\n",
    "- BLEU 4: 0.04589972764144636"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "See `translate_deploy.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
