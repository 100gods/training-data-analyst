{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- check in as git branch\n",
    "- make label offset by one from decode input (end character = 2, probably put reserved char for last thing)\n",
    "- establish benchmark on: log loss, bleu score, manual inspection\n",
    "- add attention (tf.keras.layers.Attention)\n",
    "- see if benchmarks improved\n",
    "- deploy for online prediction\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "- Attention Keras: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "- TF NMT: https://colab.sandbox.google.com/drive/1R4Hxvzf1a6H95N2sjh5_lVRat_59Zxlx#scrollTo=ddefjBMa3jF0\n",
    "\n",
    "#### completed\n",
    "\n",
    "- make reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__) # 2.0.0-beta0\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t多Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. lower case\n",
    "2. add space between puncation and words\n",
    "3. replace tokens that aren't a-z or punctation with space\n",
    "4. add \\<start> and \\<end> tokens\n",
    "5. tokenize \n",
    "6. pad to length of longest sentence (post-pad)\n",
    "7. convert to tf.data dataset\n",
    "8. shuffle and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit size to 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 3000 #30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2400, 600, 600)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "38 ----> nos\n",
      "87 ----> vamos\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "14 ----> we\n",
      "22 ----> re\n",
      "249 ----> going\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0613 20:41:12.692818 140493494408960 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((input_tensor_train, target_tensor_train), target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(((input_tensor_val, target_tensor_val), target_tensor_val))\n",
    "eval_dataset = eval_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=41, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   1,   13,   31,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   25, 1399,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   15,  204,    3,    2,    0,    0,    0,    0,    0,    0]],\n",
       "       dtype=int32)>, <tf.Tensor: id=45, shape=(3, 8), dtype=int32, numpy=\n",
       " array([[  1,   6,   8,  68,   3,   2,   0,   0],\n",
       "        [  1,   4, 504,  11,   3,   2,   0,   0],\n",
       "        [  1,   4, 173,  19,   3,   2,   0,   0]], dtype=int32)>, <tf.Tensor: id=49, shape=(3, 8), dtype=int32, numpy=\n",
       " array([[  1,   6,   8,  68,   3,   2,   0,   0],\n",
       "        [  1,   4, 504,  11,   3,   2,   0,   0],\n",
       "        [  1,   4, 173,  19,   3,   2,   0,   0]], dtype=int32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_encoder_input_batch, example_decoder_input_batch), example_target_batch = next(iter(train_dataset))\n",
    "example_encoder_input_batch[:3], example_decoder_input_batch[:3], example_target_batch[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 11, 256)      488704      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 8, 256)       235008      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 11, 1024), ( 3938304     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 8, 1024), (N 3938304     embedding_1[0][0]                \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8, 918)       940950      gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,541,270\n",
      "Trainable params: 9,541,270\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 2.8867 - val_loss: 1.7908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc68a44b9e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# Encoder\n",
    "#encoder_inputs = tf.keras.layers.Input(shape=(max_length_inp,))\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_length_inp,),name=\"encoder_input\")\n",
    "encoder_inputs_embedded = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim,input_length=max_length_inp)(encoder_inputs)\n",
    "encoder_outputs, encoder_state = tf.keras.layers.GRU(\n",
    "     units = 1024,\n",
    "     return_sequences=True,\n",
    "     return_state=True, # what is recurrent_initializer?\n",
    "     recurrent_initializer='glorot_uniform')(encoder_inputs_embedded)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_length_targ,),name=\"decoder_input\")\n",
    "decoder_inputs_embedded = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim,input_length=max_length_targ)(decoder_inputs)\n",
    "decoder_outputs, _ = tf.keras.layers.GRU(\n",
    "    units = 1024,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer='glorot_uniform')(decoder_inputs_embedded,initial_state=encoder_state)\n",
    "\n",
    "# Classifier (take each intermediate hidden state and predict word)\n",
    "predictions = tf.keras.layers.Dense(vocab_tar_size, activation='softmax')(decoder_outputs)\n",
    "\n",
    "# Model definition\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[encoder_inputs,decoder_inputs], outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "model.fit(train_dataset,validation_data=eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
