{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: TF 2.0 Dependency\n",
    "\n",
    "This code relies on TF 2.0, which AI Platform online prediction doesn't support yet. The code works locally, and the model deploys, but upon prediction it fails, presumably because the AI Platform nodes are running TF 1.13.\n",
    "\n",
    "We should re-test this notebook once TF 2.0 is support for online prediction. With any luck it will work without any further changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'vijays-sandbox'\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "MODEL_NAME = 'translate'\n",
    "VERSION_NAME = 'v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Model Artifacts to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://translate_models/baseline/encoder_model.h5 [Content-Type=application/octet-stream]...\n",
      "Copying file://translate_models/baseline/decoder_model.h5 [Content-Type=application/octet-stream]...\n",
      "\\\n",
      "Operation completed over 2 objects/63.4 MiB.                                     \n",
      "Copying file://translate_models/baseline/encoder_tokenizer.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://translate_models/baseline/decoder_tokenizer.pkl [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 2 objects/652.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $MODEL_PATH/encoder_model.h5 $MODEL_PATH/decoder_model.h5 gs://$BUCKET/translate/model/\n",
    "!gsutil cp $MODEL_PATH/encoder_tokenizer.pkl $MODEL_PATH/decoder_tokenizer.pkl gs://$BUCKET/translate/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Predictor Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor.py\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "MAX_TRANSLATE_LENGTH = 11 \n",
    "\n",
    "class TranslatePredictor(object):\n",
    "    def __init__(self, encoder_model, encoder_tokenizer, \n",
    "                 decoder_model, decoder_tokenizer):\n",
    "      self.encoder_model = encoder_model\n",
    "      self.encoder_tokenizer = encoder_tokenizer\n",
    "      self.decoder_model = decoder_model\n",
    "      self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "    def _unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def _preprocess_sentence(self, w):\n",
    "        w = self._unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    def _tokenize(self, lang,lang_tokenizer=None):\n",
    "      if lang_tokenizer==None:\n",
    "          lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "              filters='')\n",
    "          lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "      tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "      tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                             padding='post')\n",
    "\n",
    "      return tensor, lang_tokenizer\n",
    "\n",
    "    def _convert(self, lang,tensor):\n",
    "        return [lang.index_word[t] if t!=0 else '' for t in tensor]\n",
    "\n",
    "    def _preprocess(self, sentences, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: a python list of of strings\n",
    "            tokenizer: keras_preprocessing.text.Tokenizer for mapping words to integers\n",
    "        \"\"\"\n",
    "        sentences = [self._preprocess_sentence(sentence) for sentence in sentences]\n",
    "        tokens, _ = self._tokenize(sentences,tokenizer)\n",
    "        return tokens\n",
    "\n",
    "    def _decode_sequences(self, input_seqs, output_tokenizer, max_decode_length=50):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_seqs: int tensor of shape (BATCH_SIZE,SEQ_LEN)\n",
    "\n",
    "        \"\"\"\n",
    "        # Encode the input as state vectors.\n",
    "        batch_size = input_seqs.shape[0]\n",
    "        states_value = self.encoder_model.predict(input_seqs)\n",
    "\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq = tf.ones([batch_size,1])\n",
    "\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        decoded_sentences = [[] for _ in range(batch_size)]\n",
    "        for i in range(max_decode_length):\n",
    "            output_tokens, decoder_state = self.decoder_model.predict(\n",
    "                [target_seq,states_value])\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=-1)\n",
    "            tokens = self._convert(output_tokenizer,sampled_token_index)\n",
    "            for j in range (batch_size):\n",
    "                decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = tf.expand_dims(tf.constant(sampled_token_index),axis=-1)\n",
    "\n",
    "            # Update states\n",
    "            states_value = decoder_state\n",
    "\n",
    "        return decoded_sentences\n",
    "    \n",
    "    def predict(self, instances, **kwargs):\n",
    "        machine_translations = self._decode_sequences(\n",
    "            self._preprocess(instances,self.encoder_tokenizer),\n",
    "            self.decoder_tokenizer,\n",
    "            MAX_TRANSLATE_LENGTH\n",
    "        )\n",
    "        return machine_translations\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        encoder_model = tf.keras.models.load_model(os.path.join(model_dir,'encoder_model.h5'))\n",
    "        decoder_model = tf.keras.models.load_model(os.path.join(model_dir,'decoder_model.h5'))\n",
    "    \n",
    "        encoder_tokenizer = pickle.load(open(os.path.join(model_dir,'encoder_tokenizer.pkl'),'rb'))\n",
    "        decoder_tokenizer = pickle.load(open(os.path.join(model_dir,'decoder_tokenizer.pkl'),'rb'))\n",
    "\n",
    "        return cls(encoder_model, encoder_tokenizer, decoder_model, decoder_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predictor Class Works Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0617 19:34:52.350484 139915477219072 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0617 19:34:52.596298 139915477219072 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the', 'storm', 'were', '<end>', '.', '<end>', '', '', '', '', ''],\n",
       " ['her', 'car', 'inspired', 'me', '.', '<end>', '', '', '', '', ''],\n",
       " ['we', 're', 'not', 'eating', '.', '<end>', '', '', '', '', ''],\n",
       " ['winter', 'is', 'coming', '.', '<end>', '', '', '', '', '', ''],\n",
       " ['winter', 'is', 'coming', 'on', '.', '<end>', '', '', '', '', ''],\n",
       " ['there', 's', 'a', 'bus', 'here', '.', '<end>', '', '', '', ''],\n",
       " ['how', 'about', 'you', 'a', 'sad', '?', '<end>', '', '', '', '']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import predictor\n",
    "\n",
    "sentences = [\n",
    "    \"El soldado actuó valientemente.\", \n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Hay una bolsa sobre el escritorio.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]\n",
    "\n",
    "predictor = predictor.TranslatePredictor.from_path(MODEL_PATH)\n",
    "predictor.predict(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Package Predictor Class and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='translate_custom_predict_code',\n",
    "    version='0.1',\n",
    "    scripts=['predictor.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing requirements to translate_custom_predict_code.egg-info/requires.txt\n",
      "writing translate_custom_predict_code.egg-info/PKG-INFO\n",
      "writing top-level names to translate_custom_predict_code.egg-info/top_level.txt\n",
      "writing dependency_links to translate_custom_predict_code.egg-info/dependency_links.txt\n",
      "reading manifest file 'translate_custom_predict_code.egg-info/SOURCES.txt'\n",
      "writing manifest file 'translate_custom_predict_code.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating translate_custom_predict_code-0.1\n",
      "creating translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "creating translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info/.ipynb_checkpoints\n",
      "copying files to translate_custom_predict_code-0.1...\n",
      "copying predictor.py -> translate_custom_predict_code-0.1\n",
      "copying setup.py -> translate_custom_predict_code-0.1\n",
      "copying translate_custom_predict_code.egg-info/PKG-INFO -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "copying translate_custom_predict_code.egg-info/SOURCES.txt -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "copying translate_custom_predict_code.egg-info/dependency_links.txt -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "copying translate_custom_predict_code.egg-info/requires.txt -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "copying translate_custom_predict_code.egg-info/top_level.txt -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info\n",
      "copying translate_custom_predict_code.egg-info/.ipynb_checkpoints/requires-checkpoint.txt -> translate_custom_predict_code-0.1/translate_custom_predict_code.egg-info/.ipynb_checkpoints\n",
      "Writing translate_custom_predict_code-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'translate_custom_predict_code-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/translate_custom_predict_code-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.4 KiB/  2.4 KiB]                                                \n",
      "Operation completed over 1 objects/2.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/translate_custom_predict_code-0.1.tar.gz gs://$BUCKET/translate/predict_code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy\n",
    "\n",
    "Warning: If you get a GCS access error, grant the 'Storage Object Viewer' role on the bucket that contains your artifacts to the service account being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) Resource in project [vijays-sandbox] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A model with the same name already exists.\n",
      "    field: model.name\n",
      "Creating version (this might take a few minutes)......done.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --regions us-central1\n",
    "\n",
    "!gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://$BUCKET/translate/model/ \\\n",
    "  --package-uris gs://$BUCKET/translate/predict_code/translate_custom_predict_code-0.1.tar.gz \\\n",
    "  --prediction-class predictor.TranslatePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Prediction failed: unknown error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0d20721ae8c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Prediction failed: unknown error."
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "instances = [\n",
    "    [\"El soldado actuó valientemente.\"], \n",
    "    [\"Su pierna mala le impidió ganar la carrera.\"]\n",
    "]\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': instances}\n",
    ").execute()\n",
    "\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "else:\n",
    "  print(response['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
