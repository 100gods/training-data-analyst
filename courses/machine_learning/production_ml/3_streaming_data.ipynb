{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Streaming Data\n",
    "\n",
    "Learning Objectives\n",
    " - Learn how to process real-time data for ML models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "It can be useful to leverage real time data in a machine learning model when making a prediction. However, doing so requires setting up a streaming data pipeline which can be non-trivial. \n",
    "\n",
    "Typically you will have the following:\n",
    " - A series of IoT devices generating and sending data from the field in real-time (in our case these are the taxis)\n",
    " - A messaging bus to that receives and temporarily stores the IoT data (in our case this is Cloud Pub/Sub)\n",
    " - A streaming processing service that subscribes to the messaging bus, windows the messages and performs data transformations on each window (in our case this is Cloud Dataflow)\n",
    " - A persistent store to keep the processed data (in our case this is BigQuery)\n",
    "\n",
    "These steps happen continuously and in real-time, and are illustrated by the blue arrows in the diagram below. \n",
    "\n",
    "Once this streaming data pipeline is established, we need to modify our model serving to leverage it. This simply means adding a call to the persistent store (BigQuery) to fetch the latest real-time data when a prediction request comes in. This flow is illustrated by the red arrows in the diagram below. \n",
    "\n",
    "<img src='assets/taxi_streaming_data.png' width='80%'>\n",
    "\n",
    "\n",
    "In this lab we will address how to process real-time data for machine learning models. We will use the same data as our previous 'taxifare' labs, but with the addition of `trips_last_5min` data as an additional feature. This is our proxy for real-time traffic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train our model with `trips_last_5min` feature\n",
    "\n",
    "In this lab, we want to show how to process real-time data for training and prediction. So, we need to retrain our previous model with this additional feature. Go through the notebook `train.ipynb`. Open and run the notebook to train and save a model. This notebook is very similar to what we did in the Introduction to Tensorflow module but note the added feature for `trips_last_5min` in the model and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Real Time Taxi Data\n",
    "\n",
    "Since we don’t actually have real-time taxi data we will synthesize it using a simple python script. The script publishes events to Google Cloud Pub/Sub.\n",
    "\n",
    "Inspect the `iot_devices.py` script in the `taxicab_traffic` folder. It is configured to send about 2,000 trip messages every five minutes with some randomness in the frequency to mimic traffic fluctuations. These numbers come from looking at the historical average of taxi ride frequency in BigQuery. \n",
    "\n",
    "In production this script would be replaced with actual taxis with IoT devices sending trip data to Cloud Pub/Sub. \n",
    "\n",
    "To execute the iot_devices.py script, launch a terminal and navigate to the `training-data-analyst/courses/machine_learning/production_ml` directory. Then run the following two commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "python ./taxicab_traffic/iot_devices.py --project=$PROJECT_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see new messages being published every 5 seconds. **Keep this terminal open** so it continues to publish events to the Pub/Sub topic. If you open [Pub/Sub in your Google Cloud Console](https://console.cloud.google.com/cloudpubsub/topic/list), you should be able to see a topic called `taxifares`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BigQuery table to collect the processed data\n",
    "\n",
    "In the next section, we will create a dataflow pipeline to write processed taxifare data to a BigQuery Table, however that table does not yet exist. Execute the following commands to create a BigQuery dataset called `taxifare` and a table within that dataset called `taxifare`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq mk --dataset taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a table called `taxifare_realtime` and set up the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq mk --table \\\n",
    " --schema trips_last_5min:INTEGER,time:TIMESTAMP \\\n",
    " taxifare.traffic_realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Streaming Dataflow Pipeline\n",
    "\n",
    "Now that we have our taxi data being pushed to Pub/Sub, and our BigQuery table set up, let’s consume the Pub/Sub data using a streaming DataFlow pipeline.\n",
    "\n",
    "The pipeline is defined in `./taxicab_trafic/streaming_count.py`. Open that file and inspect it. \n",
    "\n",
    "There are 5 transformations being applied:\n",
    " - Read from PubSub\n",
    " - Window the messages\n",
    " - Count number of messages in the window\n",
    " - Format the count for BigQuery\n",
    " - Write results to BigQuery\n",
    "\n",
    "For the second transform, we specify a sliding window that is 5 minutes long, and recalculate values every 15 seconds. \n",
    "\n",
    "In a new terminal, launch the dataflow pipeline using the command below. You can change the `BUCKET` variable, if necessary. Here it is assumed to be your `PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$PROJECT_ID # CHANGE AS NECESSARY \n",
    "python streaming_count.py \\\n",
    "\t--input_topic taxi_rides \\\n",
    "\t--runner=DataflowRunner \\\n",
    "\t--project=$PROJECT_ID \\\n",
    "\t--temp_location=gs://$BUCKET/dataflow_streaming\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've submitted the command above you can examine the progress of that job in the [Dataflow section of Cloud console](https://console.cloud.google.com/dataflow). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few moments, you should also see new data written to your BigQuery table as well. \n",
    "\n",
    "Re-run the query periodically to observe new data streaming in! You should see a new row every 15 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `taxifare.traffic_realtime`\n",
    "ORDER BY\n",
    "  time DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions from the new data\n",
    "\n",
    "To make online predictions, we'll take advantage of [AI Platforms Custom Prediction Routines](https://cloud.google.com/ml-engine/docs/tensorflow/custom-prediction-routines) which allows us to execute custom python code in response to every online prediction request. There are 5 steps to creating a custom prediction routine:\n",
    "\n",
    "1. Upload Model Artifacts to GCS\n",
    "2. Implement Predictor interface \n",
    "3. Package the prediction code and dependencies\n",
    "4. Deploy\n",
    "5. Invoke API\n",
    "\n",
    "In the rest of the lab, we'll referece the model we trained and deployed from the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'munn-sandbox'\n",
    "BUCKET = 'munn-sandbox'\n",
    "\n",
    "MODEL_BASE = \"./export/savedmodel\"\n",
    "MODEL_PATH = os.path.join(MODEL_BASE,os.listdir(MODEL_BASE)[-1])\n",
    "MODEL_NAME = 'taxifare'\n",
    "VERSION_NAME = 'dnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(export_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll upload our model artifacts to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r $MODEL_PATH/* gs://$BUCKET/taxifare/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create our predictor interface. This will create an object `TaxifarePredictor`. This tells AI Platform how to load the model artifacts, and is where we specify our custom prediction code. By calling `.predict` we will make predictions on the most recent instances collected from our real-time dataset in BigQuery.\n",
    "\n",
    "Note: the correct PROJECT_ID will automatically be inserted using the bash `sed` command in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predictor.py\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "\n",
    "PROJECT_ID = 'will_be_replaced'\n",
    "\n",
    "class TaxifarePredictor(object):\n",
    "    def __init__(self, predict_fn):\n",
    "      self.predict_fn = predict_fn   \n",
    "    \n",
    "    def predict(self, instances, **kwargs):\n",
    "        bq = bigquery.Client(PROJECT_ID)\n",
    "        query_string = \"\"\"\n",
    "        SELECT\n",
    "          *\n",
    "        FROM\n",
    "          `taxifare.traffic_realtime`\n",
    "        ORDER BY\n",
    "          time DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        trips = bq.query(query_string).to_dataframe()['trips_last_5min'][0]\n",
    "        instances['trips_last_5min'] = [trips for _ in range(len(list(instances.items())[0][1]))]\n",
    "        predictions = self.predict_fn(instances)\n",
    "        return predictions['predictions'].tolist() # convert to list so it is JSON serialiable (requirement)\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        # predict_fn = tf.contrib.predictor.from_saved_model(model_dir,'predict')\n",
    "        loaded_model = tf.saved_model.load(export_dir=model_dir)\n",
    "        return cls(predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "sed -i -e \"s/will_be_replaced/${PROJECT_ID}/g\" predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
