{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training at scale with AI Platform Training Service\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to organize your training code into a Python package\n",
    "  1. Train your model using cloud infrastructure via Google Cloud AI Platform Training Service\n",
    "  1. (optional) Learn how to run your training package using Docker containers and push training Docker images on a Docker registry\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll make the jump from training locally, to do training in the cloud. We'll take advantage of Google Cloud's [AI Platform Training Service](https://cloud.google.com/ai-platform/). \n",
    "\n",
    "AI Platform Training Service is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handled seamlessly by the managed service for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your project name and bucket name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "PROJECT = \"munn-sandbox\"\n",
    "BUCKET = \"munn-sandbox\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = \"2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with AI Platform Training Service\n",
    "In order to make our code compatible with AI Platform Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a trainer Python package\n",
    "4. Submit training job with `gcloud` to train on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Google Cloud Storage (GCS)\n",
    "\n",
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS.\n",
    "\n",
    "To do this run the notebook [0_export_data_from_bq_to_gcs.ipynb](./0_export_data_from_bq_to_gcs.ipynb), which will export the taxifare data from BigQuery directly into a GCS bucket. If all ran smoothly, you should be able to list the data bucket by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/data/taxi-train-000000000000.csv\n",
      "gs://munn-sandbox/taxifare/data/taxi-valid-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move code into a python package\n",
    "\n",
    "\n",
    "The first thing to do is to convert you training code snippets into a regular Python package that we will then `pip install` into the Docker container. \n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the package directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our package directory contains 3 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  model.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  task.py\n"
     ]
    }
   ],
   "source": [
    "ls ./taxifare/trainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paste existing code into model.py\n",
    "\n",
    "A Python package requires our code to be in a .py file, as opposed to notebook cells. So, we simply copy and paste our existing code for the previous notebook into a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we write the content of the cell into `model.py` packaging the model we \n",
    "developed in the previous labs so that we can deploy it to AI Platform Training Service.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./taxifare/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/trainer/model.py  #TODO 1\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "        'fare_amount',\n",
    "        'pickup_datetime',\n",
    "        'pickup_longitude',\n",
    "        'pickup_latitude',\n",
    "        'dropoff_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'passenger_count',\n",
    "        'key',\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "DAYS = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def parse_datetime(s):\n",
    "    if type(s) is not str:\n",
    "        s = s.numpy().decode('utf-8')\n",
    "    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff*londiff + latdiff*latdiff)\n",
    "\n",
    "\n",
    "def get_dayofweek(s):\n",
    "    ts = parse_datetime(s)\n",
    "    return DAYS[ts.weekday()]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dayofweek(ts_in):\n",
    "    return tf.map_fn(\n",
    "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string),\n",
    "        ts_in\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def fare_thresh(x):\n",
    "    return 60 * activations.relu(x)\n",
    "\n",
    "\n",
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed['pickup_datetime']\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = layers.Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name='scale_{}'.format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = layers.Lambda(\n",
    "            lambda x: (x - 37)/8.0,\n",
    "            name='scale_{}'.format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "\n",
    "    # Adding Euclidean dist (no need to be accurate: NN will calibrate it)\n",
    "    transformed['euclidean'] = layers.Lambda(euclidean, name='euclidean')([\n",
    "        inputs['pickup_longitude'],\n",
    "        inputs['pickup_latitude'],\n",
    "        inputs['dropoff_longitude'],\n",
    "        inputs['dropoff_latitude']\n",
    "    ])\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "\n",
    "    # hour of day from timestamp of form '2010-02-08 09:17:00+00:00'\n",
    "    transformed['hourofday'] = layers.Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "            tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32),\n",
    "        name='hourofday'\n",
    "    )(inputs['pickup_datetime'])\n",
    "    feature_columns['hourofday'] = fc.indicator_column(\n",
    "        fc.categorical_column_with_identity(\n",
    "            'hourofday', num_buckets=24))\n",
    "\n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = fc.bucketized_column(\n",
    "        feature_columns['pickup_latitude'], latbuckets)\n",
    "    b_dlat = fc.bucketized_column(\n",
    "            feature_columns['dropoff_latitude'], latbuckets)\n",
    "    b_plon = fc.bucketized_column(\n",
    "            feature_columns['pickup_longitude'], lonbuckets)\n",
    "    b_dlon = fc.bucketized_column(\n",
    "            feature_columns['dropoff_longitude'], lonbuckets)\n",
    "    ploc = fc.crossed_column(\n",
    "            [b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = fc.crossed_column(\n",
    "            [b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    feature_columns['pickup_and_dropoff'] = fc.embedding_column(\n",
    "            pd_pair, 100)\n",
    "\n",
    "    return transformed, feature_columns\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize):\n",
    "    # input layer is all float except for pickup_datetime which is a string\n",
    "    STRING_COLS = ['pickup_datetime']\n",
    "    NUMERIC_COLS = (\n",
    "            set(CSV_COLUMNS) - set([LABEL_COLUMN, 'key']) - set(STRING_COLS)\n",
    "    )\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in STRING_COLS\n",
    "    })\n",
    "\n",
    "    # transforms\n",
    "    transformed, feature_columns = transform(\n",
    "        inputs, NUMERIC_COLS, STRING_COLS, nbuckets=nbuckets)\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = layers.Dense(nodes, activation='relu', name='h{}'.format(layer))(x)\n",
    "    output = layers.Dense(1, name='fare')(x)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse, 'mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams['batch_size']\n",
    "    eval_data_path = hparams['eval_data_path']\n",
    "    nnsize = hparams['nnsize']\n",
    "    nbuckets = hparams['nbuckets']\n",
    "    num_evals = hparams['num_evals']\n",
    "    num_examples_to_train_on = hparams['num_examples_to_train_on']\n",
    "    output_dir = hparams['output_dir']\n",
    "    train_data_path = hparams['train_data_path']\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    savedmodel_dir = os.path.join(output_dir, 'export/savedmodel')\n",
    "    model_export_path = os.path.join(savedmodel_dir, timestamp)\n",
    "    checkpoint_path = os.path.join(output_dir, 'checkpoints')\n",
    "    tensorboard_path = os.path.join(output_dir, 'tensorboard')\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nbuckets, nnsize)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    tf.saved_model.save(model, model_export_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify code to read data from and write checkpoint files to GCS \n",
    "\n",
    "If you look closely above, you'll notice a new function, `train_and_evaluate` that wraps the code that actually trains the model. This allows us to parametrize the training by passing a dictionary of parameters to this function (e.g, `batch_size`, `num_examples_to_train_on`, `train_data_path` etc.)\n",
    "\n",
    "This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parametrizing allows us to use the same code for both.\n",
    "\n",
    "We specify these parameters at run time via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with those params. This is the job of the `task.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/task.py # TODO 1\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[32, 8]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help=\"Number of buckets to divide lat and lon with\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    hparams.pop(\"job-dir\", None)\n",
    "\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trainer module package locally\n",
    "\n",
    "Now we can test our training code locally as follows using the local test data. We'll run a very small training job over a single file with a small batch size and one eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_datetime (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "euclidean (Lambda)              (None,)              0           pickup_longitude[0][0]           \n",
      "                                                                 pickup_latitude[0][0]            \n",
      "                                                                 dropoff_longitude[0][0]          \n",
      "                                                                 dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hourofday (Lambda)              (None,)              0           pickup_datetime[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 130)          1000000     scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 euclidean[0][0]                  \n",
      "                                                                 hourofday[0][0]                  \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "h0 (Dense)                      (None, 32)           4192        dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "h1 (Dense)                      (None, 8)            264         h0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fare (Dense)                    (None, 1)            9           h1[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 1,004,465\n",
      "Trainable params: 1,004,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train for 20 steps\n",
      "\n",
      "Epoch 00001: saving model to ./taxifare-model/checkpoints\n",
      "20/20 - 2s - loss: 270.3467 - rmse: 14.7056 - mse: 270.3466 - val_loss: 214.6409 - val_rmse: 13.7667 - val_mse: 213.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-16 17:31:20.279796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-16 17:31:21.572833: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-04-16 17:31:21.574646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "2020-04-16 17:31:22.621854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-04-16 17:31:24.788941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.789723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-04-16 17:31:24.789760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-16 17:31:24.789803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-16 17:31:24.791765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-04-16 17:31:24.792085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-04-16 17:31:24.794309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-04-16 17:31:24.795423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-04-16 17:31:24.795481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-04-16 17:31:24.795643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.796484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.797171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-04-16 17:31:24.806277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-04-16 17:31:24.806793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562dd96c78d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-16 17:31:24.806826: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-04-16 17:31:24.853576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.854460: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562dd973dd70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-16 17:31:24.854499: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-04-16 17:31:24.854736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.855491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-04-16 17:31:24.855548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-16 17:31:24.855585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-16 17:31:24.855648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-04-16 17:31:24.855699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-04-16 17:31:24.855742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-04-16 17:31:24.855799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-04-16 17:31:24.855837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-04-16 17:31:24.855932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.856774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:24.857486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-04-16 17:31:24.857626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-16 17:31:25.543808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-04-16 17:31:25.543879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2020-04-16 17:31:25.543890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2020-04-16 17:31:25.544263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:25.545093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-16 17:31:25.545833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10714 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2020-04-16 17:31:27.120286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-16 17:31:27.319521: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\n",
      "2020-04-16 17:31:27.319592: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs\n",
      "2020-04-16 17:31:27.320560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\n",
      "2020-04-16 17:31:27.421199: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2020-04-16 17:31:27.421950: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2020-04-16 17:31:27.431042: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\n",
      "2020-04-16 17:31:27.431106: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\n",
      "2020-04-16 17:31:28.115157: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[model/dense_features/dropoff_latitude_bucketized_X_dropoff_longitude_bucketized_X_pickup_latitude_bucketized_X_pickup_longitude_bucketized_embedding/dropoff_latitude_bucketized_X_dropoff_longitude_bucketized_X_pickup_latitude_bucketized_X_pickup_longitude_bucketized_embedding_weights/Prod/_102]]\n",
      "2020-04-16 17:31:28.115338: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2020-04-16 17:31:29.446340: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=./taxifare/tests/data/taxi-valid*\n",
    "TRAIN_DATA_PATH=./taxifare/tests/data/taxi-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "    \n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--nnsize 32 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your training package on Cloud AI Platform\n",
    "\n",
    "Once the code works in standalone mode locally, you can run it on Cloud AI Platform. To submit to the Cloud we use [`gcloud ai-platform jobs submit training [jobname]`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) and simply specify some additional parameters for AI Platform Training Service:\n",
    "- jobid: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/ml-engine/docs/tensorflow/regions) for supported AI Platform Training Service regions\n",
    "\n",
    "The arguments before `-- \\` are for AI Platform Training Service.\n",
    "The arguments after `-- \\` are sent to our `task.py`.\n",
    "\n",
    "Because this is on the entire dataset, it will take a while. You can monitor the job from the GCP console in the [Cloud AI Platform section](https://pantheon.corp.google.com/ai-platform/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/trained_model us-central1 taxifare_200416_173443\n",
      "jobId: taxifare_200416_173443\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/#1587057967723620...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/checkpoint#1587058478953714...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/checkpoints.index#1587058478502530...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/checkpoints.data-00000-of-00001#1587058478327126...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/#1587057967936084...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/events.out.tfevents.1587057696.cmle-training-7879419774099334716.1.715.v2#1587058479152337...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/events.out.tfevents.1587057967.cmle-training-798550816293841297.396.715.v2#1587058478780809...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/plugins/profile/2020-04-16_17-26-24/#1587057985533417...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/events.out.tfevents.1587057985.cmle-training-798550816293841297.profile-empty#1587057985718589...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/plugins/profile/2020-04-16_17-26-24/local.trace#1587057986925163...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/plugins/#1587057985164137...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/validation/events.out.tfevents.1587057711.cmle-training-7879419774099334716.1.11284.v2#1587058479309463...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/train/plugins/profile/#1587057985355138...\n",
      "Removing gs://munn-sandbox/taxifare/trained_model/tensorboard/validation/events.out.tfevents.1587057979.cmle-training-798550816293841297.396.11284.v2#1587058478996338...\n",
      "/ [14/14 objects] 100% Done                                                     \n",
      "Operation completed over 14 objects.                                             \n",
      "Job [taxifare_200416_173443] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe taxifare_200416_173443\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs taxifare_200416_173443\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# TODO 2\n",
    "\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model\n",
    "JOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOBID \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=taxifare/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --region=${REGION} \\\n",
    "    -- \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data_path $TRAIN_DATA_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "    --num_evals $NUM_EVALS \\\n",
    "    --nbuckets $NBUCKETS \\\n",
    "    --nnsize $NNSIZE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Run your training package using Docker container\n",
    "\n",
    "AI Platform Training also supports training in custom containers, allowing users to bring their own Docker containers with any pre-installed ML framework or algorithm to run on AI Platform Training. \n",
    "\n",
    "In this last section, we'll see how to submit a Cloud training job using a customized Docker image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containerizing our `./taxifare/trainer` package involves 3 steps:\n",
    "\n",
    "* Writing a Dockerfile in `./taxifare`\n",
    "* Building the Docker image\n",
    "* Pusing it to the Google Cloud container registry in our GCP project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dockerfile` specifies\n",
    "1. How the container needs to be provisioned so that all the dependencies in our code are satisfied\n",
    "2. Where to copy our trainer Package in the container and how to install it (`pip install /trainer`)\n",
    "3. What command to run when the container is ran (the `ENTRYPOINT` line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./taxifare/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "\n",
    "COPY . /code\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --yes python3-pip && \\\n",
    "    pip3 install /code \n",
    "\n",
    "RUN python3 -m pip install --upgrade --quiet cloudml-hypertune\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/code/trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"asia.gcr.io\": \"gcloud\", \n",
      "    \"staging-k8s.gcr.io\": \"gcloud\", \n",
      "    \"us.gcr.io\": \"gcloud\", \n",
      "    \"gcr.io\": \"gcloud\", \n",
      "    \"marketplace.gcr.io\": \"gcloud\", \n",
      "    \"eu.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for all GCR repositories.\n",
      "\u001b[1;33mWARNING:\u001b[0m A long list of credential helpers may cause delays running 'docker build'. We recommend passing the registry name to configure only the registry you are using.\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  133.6kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
      " ---> 4f3009408e35\n",
      "Step 2/5 : COPY . /code\n",
      " ---> f25b205d67e0\n",
      "Step 3/5 : RUN apt-get update &&     apt-get install --yes python3-pip &&     pip3 install /code\n",
      " ---> Running in c53b7b30dfff\n",
      "Get:1 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [3724 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6381 B]\n",
      "Get:5 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [2077 B]\n",
      "Get:6 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [111 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [44.6 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8213 B]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [889 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [839 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1183 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [59.0 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1372 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [12.6 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4247 B]\n",
      "Fetched 17.9 MB in 4s (5041 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6-dev python-pip-whl\n",
      "  python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography\n",
      "  python3-dev python3-distutils python3-idna python3-keyring\n",
      "  python3-keyrings.alt python3-lib2to3 python3-pkg-resources\n",
      "  python3-secretstorage python3-setuptools python3-six python3-wheel\n",
      "  python3-xdg python3.6-dev\n",
      "Suggested packages:\n",
      "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
      "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
      "  python-secretstorage-doc python-setuptools-doc\n",
      "The following NEW packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6-dev python-pip-whl\n",
      "  python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography\n",
      "  python3-dev python3-distutils python3-idna python3-keyring\n",
      "  python3-keyrings.alt python3-lib2to3 python3-pip python3-pkg-resources\n",
      "  python3-secretstorage python3-setuptools python3-six python3-wheel\n",
      "  python3-xdg python3.6-dev\n",
      "0 upgraded, 23 newly installed, 0 to remove and 44 not upgraded.\n",
      "Need to get 48.6 MB of archives.\n",
      "After this operation, 90.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-lib2to3 all 3.6.9-1~18.04 [77.4 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-distutils all 3.6.9-1~18.04 [144 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-python all 3.20180325ubuntu2 [89.2 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libexpat1-dev amd64 2.2.5-3ubuntu0.2 [122 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-dev amd64 3.6.9-1~18.04 [44.8 MB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3-dev amd64 3.6.7-1~18.04 [7328 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.1 [1653 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.3 [221 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-dev amd64 3.6.9-1~18.04 [508 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-dev amd64 3.6.7-1~18.04 [1288 B]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.1 [114 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-xdg all 0.25-4ubuntu1 [31.4 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 48.6 MB in 4s (12.3 MB/s)\n",
      "Selecting previously unselected package python3-lib2to3.\n",
      "(Reading database ... 86696 files and directories currently installed.)\n",
      "Preparing to unpack .../00-python3-lib2to3_3.6.9-1~18.04_all.deb ...\n",
      "Unpacking python3-lib2to3 (3.6.9-1~18.04) ...\n",
      "Selecting previously unselected package python3-distutils.\n",
      "Preparing to unpack .../01-python3-distutils_3.6.9-1~18.04_all.deb ...\n",
      "Unpacking python3-distutils (3.6.9-1~18.04) ...\n",
      "Selecting previously unselected package dh-python.\n",
      "Preparing to unpack .../02-dh-python_3.20180325ubuntu2_all.deb ...\n",
      "Unpacking dh-python (3.20180325ubuntu2) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../03-libexpat1-dev_2.2.5-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libpython3.6-dev:amd64.\n",
      "Preparing to unpack .../04-libpython3.6-dev_3.6.9-1~18.04_amd64.deb ...\n",
      "Unpacking libpython3.6-dev:amd64 (3.6.9-1~18.04) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../05-libpython3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "Preparing to unpack .../06-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
      "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Selecting previously unselected package python3-asn1crypto.\n",
      "Preparing to unpack .../07-python3-asn1crypto_0.24.0-1_all.deb ...\n",
      "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
      "Selecting previously unselected package python3-cffi-backend.\n",
      "Preparing to unpack .../08-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
      "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
      "Selecting previously unselected package python3-crypto.\n",
      "Preparing to unpack .../09-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
      "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
      "Selecting previously unselected package python3-idna.\n",
      "Preparing to unpack .../10-python3-idna_2.6-1_all.deb ...\n",
      "Unpacking python3-idna (2.6-1) ...\n",
      "Selecting previously unselected package python3-six.\n",
      "Preparing to unpack .../11-python3-six_1.11.0-2_all.deb ...\n",
      "Unpacking python3-six (1.11.0-2) ...\n",
      "Selecting previously unselected package python3-cryptography.\n",
      "Preparing to unpack .../12-python3-cryptography_2.1.4-1ubuntu1.3_amd64.deb ...\n",
      "Unpacking python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
      "Selecting previously unselected package python3.6-dev.\n",
      "Preparing to unpack .../13-python3.6-dev_3.6.9-1~18.04_amd64.deb ...\n",
      "Unpacking python3.6-dev (3.6.9-1~18.04) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../14-python3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking python3-dev (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python3-secretstorage.\n",
      "Preparing to unpack .../15-python3-secretstorage_2.3.1-2_all.deb ...\n",
      "Unpacking python3-secretstorage (2.3.1-2) ...\n",
      "Selecting previously unselected package python3-keyring.\n",
      "Preparing to unpack .../16-python3-keyring_10.6.0-1_all.deb ...\n",
      "Unpacking python3-keyring (10.6.0-1) ...\n",
      "Selecting previously unselected package python3-keyrings.alt.\n",
      "Preparing to unpack .../17-python3-keyrings.alt_3.0-1_all.deb ...\n",
      "Unpacking python3-keyrings.alt (3.0-1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../18-python3-pip_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
      "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "Preparing to unpack .../19-python3-pkg-resources_39.0.1-2_all.deb ...\n",
      "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../20-python3-setuptools_39.0.1-2_all.deb ...\n",
      "Unpacking python3-setuptools (39.0.1-2) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../21-python3-wheel_0.30.0-0.2_all.deb ...\n",
      "Unpacking python3-wheel (0.30.0-0.2) ...\n",
      "Selecting previously unselected package python3-xdg.\n",
      "Preparing to unpack .../22-python3-xdg_0.25-4ubuntu1_all.deb ...\n",
      "Unpacking python3-xdg (0.25-4ubuntu1) ...\n",
      "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Setting up python3-cffi-backend (1.11.5-1) ...\n",
      "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
      "Setting up python3-idna (2.6-1) ...\n",
      "Setting up python3-xdg (0.25-4ubuntu1) ...\n",
      "Setting up python3-six (1.11.0-2) ...\n",
      "Setting up python3-wheel (0.30.0-0.2) ...\n",
      "Setting up python3-pkg-resources (39.0.1-2) ...\n",
      "Setting up python3-asn1crypto (0.24.0-1) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Setting up python3-lib2to3 (3.6.9-1~18.04) ...\n",
      "Setting up python3-distutils (3.6.9-1~18.04) ...\n",
      "Setting up python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
      "Setting up python3-keyrings.alt (3.0-1) ...\n",
      "Setting up libpython3.6-dev:amd64 (3.6.9-1~18.04) ...\n",
      "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Setting up python3-setuptools (39.0.1-2) ...\n",
      "Setting up python3.6-dev (3.6.9-1~18.04) ...\n",
      "Setting up python3-secretstorage (2.3.1-2) ...\n",
      "Setting up dh-python (3.20180325ubuntu2) ...\n",
      "Setting up libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Setting up python3-keyring (10.6.0-1) ...\n",
      "Setting up python3-dev (3.6.7-1~18.04) ...\n",
      "\u001b[91mERROR: Directory '/code' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n",
      "\u001b[0mThe push refers to repository [gcr.io/munn-sandbox/taxifare_training_container]\n",
      "f6ffd01ef4c2: Preparing\n",
      "f751b6b05c58: Preparing\n",
      "400a4e3991e9: Preparing\n",
      "41b379bf2eb3: Preparing\n",
      "77db3bd6efb2: Preparing\n",
      "07fd8d677a40: Preparing\n",
      "992939e921d8: Preparing\n",
      "a304fb96c494: Preparing\n",
      "431f13f6088f: Preparing\n",
      "f63b09c90bb4: Preparing\n",
      "eac1f876522f: Preparing\n",
      "391d3bae9f0a: Preparing\n",
      "0d69a3a385f0: Preparing\n",
      "dab306330749: Preparing\n",
      "a6c05cf3b0f4: Preparing\n",
      "1c0e7affc630: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "0d69a3a385f0: Waiting\n",
      "dab306330749: Waiting\n",
      "a6c05cf3b0f4: Waiting\n",
      "1c0e7affc630: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "07fd8d677a40: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "431f13f6088f: Waiting\n",
      "f63b09c90bb4: Waiting\n",
      "eac1f876522f: Waiting\n",
      "992939e921d8: Waiting\n",
      "a304fb96c494: Waiting\n",
      "391d3bae9f0a: Waiting\n",
      "f751b6b05c58: Layer already exists\n",
      "400a4e3991e9: Layer already exists\n",
      "f6ffd01ef4c2: Layer already exists\n",
      "41b379bf2eb3: Layer already exists\n",
      "77db3bd6efb2: Layer already exists\n",
      "992939e921d8: Layer already exists\n",
      "431f13f6088f: Layer already exists\n",
      "07fd8d677a40: Layer already exists\n",
      "f63b09c90bb4: Layer already exists\n",
      "a304fb96c494: Layer already exists\n",
      "dab306330749: Layer already exists\n",
      "0d69a3a385f0: Layer already exists\n",
      "eac1f876522f: Layer already exists\n",
      "391d3bae9f0a: Layer already exists\n",
      "a6c05cf3b0f4: Layer already exists\n",
      "1c0e7affc630: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "latest: digest: sha256:edece4e57ec682d4b5a38d30cfa7e3c5ae9c2176358923a545a2c9e0a4859dc4 size: 4507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The command '/bin/sh -c apt-get update &&     apt-get install --yes python3-pip &&     pip3 install /code' returned a non-zero code: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "PROJECT_DIR=$(cd ./taxifare && pwd)\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "DOCKERFILE=$PROJECT_DIR/Dockerfile\n",
    "IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_NAME\n",
    "\n",
    "docker build $PROJECT_DIR -f $DOCKERFILE -t $IMAGE_URI\n",
    "\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** If you prefer to build the container image from the command line, we have written a script for that `./taxifare/scripts/build.sh`. This script reads its configuration from the file `./taxifare/scripts/env.sh`. You can configure these arguments the way you want in that file. You can also simply type `make build` from within `./taxifare` to build the image (which will invoke the build script). Similarly, we wrote the script `./taxifare/scripts/push.sh` to push the Docker image, which you can also trigger by typing `make push` from within `./taxifare`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using a custom container on AI Platform\n",
    "\n",
    "To submit to the Cloud we use [`gcloud ai-platform jobs submit training [jobname]`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) and simply specify some additional parameters for AI Platform Training Service:\n",
    "- jobname: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- master-image-uri: The uri of the Docker image we pushed in the Google Cloud registry\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/ml-engine/docs/tensorflow/regions) for supported AI Platform Training Service regions\n",
    "\n",
    "\n",
    "The arguments before `-- \\` are for AI Platform Training Service.\n",
    "The arguments after `-- \\` are sent to our `task.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your job and view logs using [cloud console](https://console.cloud.google.com/mlengine/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/trained_model us-central1 taxifare_200416_171705\n",
      "jobId: taxifare_200416_171705\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [taxifare_200416_171705] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe taxifare_200416_171705\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs taxifare_200416_171705\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$PROJECT_ID\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model\n",
    "JOBID=taxifare_container_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# AI-Platform machines to use for training\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "SCALE_TIER=CUSTOM\n",
    "\n",
    "# GCS paths.\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_NAME\n",
    "\n",
    "gcloud beta ai-platform jobs submit training $JOBID \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --region=$REGION \\\n",
    "   --master-image-uri=$IMAGE_URI \\\n",
    "   --master-machine-type=$MACHINE_TYPE \\\n",
    "   --scale-tier=$SCALE_TIER \\\n",
    "  -- \\\n",
    "  --eval_data_path $EVAL_DATA_PATH \\\n",
    "  --output_dir $OUTDIR \\\n",
    "  --train_data_path $TRAIN_DATA_PATH \\\n",
    "  --batch_size $BATCH_SIZE \\\n",
    "  --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "  --num_evals $NUM_EVALS \\\n",
    "  --nbuckets $NBUCKETS \\\n",
    "  --nnsize $NNSIZE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** If you prefer submitting your jobs for training on the AI-platform using the command line, we have written the `./taxifare/scripts/submit.sh` for you (that you can also invoke using `make submit` from within `./taxifare`). As the other scripts, it reads it configuration variables from `./taxifare/scripts/env.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
