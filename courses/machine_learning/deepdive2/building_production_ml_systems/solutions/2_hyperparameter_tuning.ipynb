{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Understand various approaches to hyperparameter tuning\n",
    "2. Automate hyperparameter tuning using AI Platform\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let's see if we can improve upon that by tuning our hyperparameters.\n",
    "\n",
    "Hyperparameters are parameters that are set *prior* to training a model, as opposed to parameters which are learned *during* training. \n",
    "\n",
    "These include learning rate and batch size, but also model design parameters such as type of activation function and number of hidden units.\n",
    "\n",
    "Here are the four most common ways to finding the ideal hyperparameters:\n",
    "1. Manual\n",
    "2. Grid Search\n",
    "3. Random Search\n",
    "4. Bayesian Optimzation\n",
    "\n",
    "**1. Manual**\n",
    "\n",
    "Traditionally, hyperparameter tuning is a manual trial and error process. A data scientist has some intution about suitable hyperparameters which they use as a starting point, then they observe the result and use that information to try a new set of hyperparameters to try to beat the existing performance. \n",
    "\n",
    "Pros\n",
    "- Educational, builds up your intuition as a data scientist\n",
    "- Inexpensive because only one trial is conducted at a time\n",
    "\n",
    "Cons\n",
    "- Requires alot of time and patience\n",
    "\n",
    "**2. Grid Search**\n",
    "\n",
    "On the other extreme we can use grid search. Define a discrete set of values to try for each hyperparameter then try every possible combination. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Gauranteed to find the best solution within the search space\n",
    "\n",
    "Cons\n",
    "- Expensive\n",
    "\n",
    "**3. Random Search**\n",
    "\n",
    "Alternatively define a range for each hyperparamter (e.g. 0-256) and sample uniformly at random from that range. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Requires less trials than Grid Search to find a good solution\n",
    "\n",
    "Cons\n",
    "- Expensive (but less so than Grid Search)\n",
    "\n",
    "**4. Bayesian Optimization**\n",
    "\n",
    "Unlike Grid Search and Random Search, Bayesian Optimization takes into account information from  past trials to select parameters for future trials. The details of how this is done is beyond the scope of this notebook, but if you're interested you can read how it works here [here](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization). \n",
    "\n",
    "Pros\n",
    "- Picks values intelligenty based on results from past trials\n",
    "- Less expensive because requires fewer trials to get a good result\n",
    "\n",
    "Cons\n",
    "- Requires sequential trials for best results, takes longer\n",
    "\n",
    "**AI Platform HyperTune**\n",
    "\n",
    "AI Platform HyperTune, powered by [Google Vizier](https://ai.google/research/pubs/pub46180), uses Bayesian Optimization by default, but [also supports](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview#search_algorithms) Grid Search and Random Search. \n",
    "\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tunining several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"munn-sandbox\"  # Replace with your PROJECT\n",
    "BUCKET = \"munn-sandbox\"  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for AI Platform\n",
    "TFVERSION = \"2.1\"                # TF version for AI Platform to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move code into python package\n",
    "\n",
    "In the [previous lab](), we moved our code into a python package for training on Cloud AI Platform. Let's just check that the files are there. You should see the following files in the `taxifare/trainer` directory:\n",
    " - `__init__.py`\n",
    " - `model.py`\n",
    " - `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Mar 25 22:16 .\n",
      "drwxr-xr-x 6 jupyter jupyter 4096 Mar 25 22:16 ..\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Mar 25 22:06 __init__.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Mar 25 22:07 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 jupyter jupyter 7356 Mar 25 22:16 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1651 Mar 25 22:16 task.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la taxifare/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    " 1. Specify the hyperparameter tuning configuration for your training job by including a HyperparameterSpec in your TrainingInput object.\n",
    "\n",
    " 2. Include the following code in your training application:\n",
    "\n",
    "  - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial.\n",
    "Add your hyperparameter metric to the summary for your graph.\n",
    "\n",
    "  - To submit a hyperparameter tuning job, we must modify `model.py` and `task.py` to expose any variables we want to tune as command line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./taxifare/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    DenseFeatures,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "        'fare_amount',\n",
    "        'pickup_datetime',\n",
    "        'pickup_longitude',\n",
    "        'pickup_latitude',\n",
    "        'dropoff_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'passenger_count',\n",
    "        'key',\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "DAYS = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def parse_datetime(s):\n",
    "    if type(s) is not str:\n",
    "        s = s.numpy().decode('utf-8')\n",
    "    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff*londiff + latdiff*latdiff)\n",
    "\n",
    "\n",
    "def get_dayofweek(s):\n",
    "    ts = parse_datetime(s)\n",
    "    return DAYS[ts.weekday()]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dayofweek(ts_in):\n",
    "    return tf.map_fn(\n",
    "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string),\n",
    "        ts_in\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def fare_thresh(x):\n",
    "    return 60 * relu(x)\n",
    "\n",
    "\n",
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed['pickup_datetime']\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name='scale_{}'.format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = Lambda(\n",
    "            lambda x: (x - 37)/8.0,\n",
    "            name='scale_{}'.format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "\n",
    "    # Adding Euclidean dist (no need to be accurate: NN will calibrate it)\n",
    "    transformed['euclidean'] = Lambda(euclidean, name='euclidean')([\n",
    "        inputs['pickup_longitude'],\n",
    "        inputs['pickup_latitude'],\n",
    "        inputs['dropoff_longitude'],\n",
    "        inputs['dropoff_latitude']\n",
    "    ])\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "\n",
    "    # hour of day from timestamp of form '2010-02-08 09:17:00+00:00'\n",
    "    transformed['hourofday'] = Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "            tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32),\n",
    "        name='hourofday'\n",
    "    )(inputs['pickup_datetime'])\n",
    "    feature_columns['hourofday'] = fc.indicator_column(\n",
    "        fc.categorical_column_with_identity(\n",
    "            'hourofday', num_buckets=24))\n",
    "\n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = fc.bucketized_column(\n",
    "        feature_columns['pickup_latitude'], latbuckets)\n",
    "    b_dlat = fc.bucketized_column(\n",
    "            feature_columns['dropoff_latitude'], latbuckets)\n",
    "    b_plon = fc.bucketized_column(\n",
    "            feature_columns['pickup_longitude'], lonbuckets)\n",
    "    b_dlon = fc.bucketized_column(\n",
    "            feature_columns['dropoff_longitude'], lonbuckets)\n",
    "    ploc = fc.crossed_column(\n",
    "            [b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = fc.crossed_column(\n",
    "            [b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    feature_columns['pickup_and_dropoff'] = fc.embedding_column(\n",
    "            pd_pair, 100)\n",
    "\n",
    "    return transformed, feature_columns\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr):\n",
    "    # input layer is all float except for pickup_datetime which is a string\n",
    "    STRING_COLS = ['pickup_datetime']\n",
    "    NUMERIC_COLS = (\n",
    "            set(CSV_COLUMNS) - set([LABEL_COLUMN, 'key']) - set(STRING_COLS)\n",
    "    )\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in STRING_COLS\n",
    "    })\n",
    "\n",
    "    # transforms\n",
    "    transformed, feature_columns = transform(\n",
    "        inputs, NUMERIC_COLS, STRING_COLS, nbuckets=nbuckets)\n",
    "    dnn_inputs = DenseFeatures(feature_columns.values())(transformed)\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, neurons in enumerate(nnsize):\n",
    "        x = Dense(neurons, activation='relu', name='h{}'.format(layer))(x)\n",
    "    output = Dense(1, name='fare')(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    \n",
    "    # TODO add in custom lr to optimizer\n",
    "    my_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, name='Adam')\n",
    "    model.compile(optimizer=my_optimizer, loss='mse', metrics=[rmse, 'mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams['batch_size'] # this would become a todo\n",
    "    eval_data_path = hparams['eval_data_path']\n",
    "    nnsize = hparams['nnsize'] # this would become a todo\n",
    "    nbuckets = hparams['nbuckets'] # this would become a todo\n",
    "    lr = hparams['lr']  # TODO\n",
    "    num_evals = hparams['num_evals']\n",
    "    num_examples_to_train_on = hparams['num_examples_to_train_on']\n",
    "    output_dir = hparams['output_dir']\n",
    "    train_data_path = hparams['train_data_path']\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    savedmodel_dir = os.path.join(output_dir, 'export/savedmodel')\n",
    "    model_export_path = os.path.join(savedmodel_dir, timestamp)\n",
    "    checkpoint_path = os.path.join(output_dir, 'checkpoints')\n",
    "    tensorboard_path = os.path.join(output_dir, 'tensorboard')\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nbuckets, nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    tensorboard_cb = TensorBoard(tensorboard_path)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    tf.saved_model.save(model, model_export_path)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # will become a TODO\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Batch size for training steps\",\n",
    "        type = int,\n",
    "        default = 32\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help = \"GCS location pattern of eval files\",\n",
    "        required = True\n",
    "    )\n",
    "    \n",
    "    # will become a TODO\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help = \"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs = \"+\",\n",
    "        type = int,\n",
    "        default=[32, 8]\n",
    "    )\n",
    "\n",
    "    # will become a TODO\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help = \"Number of buckets to divide lat and lon with\",\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    \n",
    "    # TODO\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        help = \"learning rate for optimizer\",\n",
    "        type = float,\n",
    "        default = 0.001\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help = \"Number of times to evaluate model on eval data training.\",\n",
    "        type = int,\n",
    "        default = 5\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help = \"Number of examples to train on.\",\n",
    "        type = int,\n",
    "        default = 100\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS location pattern of train files containing eval URLs\",\n",
    "        required = True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    \n",
    "    # Append trial_id to path so trials don't overwrite each other\n",
    "    hparams[\"output_dir\"] = os.path.join(\n",
    "        args[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )     \n",
    "    hparams.pop(\"job-dir\", None)\n",
    "\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config.yaml file\n",
    "\n",
    "Specify the hyperparameter tuning configuration for your training job\n",
    "Create a HyperparameterSpec object to hold the hyperparameter tuning configuration for your training job, and add the HyperparameterSpec as the hyperparameters object in your TrainingInput object.\n",
    "\n",
    "In your HyperparameterSpec, set the hyperparameterMetricTag to a value representing your chosen metric. If you don't specify a hyperparameterMetricTag, AI Platform Training looks for a metric with the name training/hptuning/metric. The following example shows how to create a configuration for a metric named metric1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hptuning_config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: BASIC\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 50\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: rmse\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: lr\n",
    "      type: DOUBLE\n",
    "      minValue: 0.0001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: nbuckets\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 25\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: batch_size\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 15\n",
    "      - 30\n",
    "      - 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report your hyperparameter metric to AI Platform Training\n",
    "\n",
    "The way to report your hyperparameter metric to the AI Platform Training service depends on whether you are using TensorFlow for training or not. It also depends on whether you are using a runtime version or a custom container for training.\n",
    "\n",
    "We recommend that your training code reports your hyperparameter metric to AI Platform Training frequently in order to take advantage of early stopping.\n",
    "\n",
    "TensorFlow with a runtime version\n",
    "If you use an AI Platform Training runtime version and train with TensorFlow, then you can report your hyperparameter metric to AI Platform Training by writing the metric to a TensorFlow summary. Use one of the following functions:\n",
    "\n",
    "tf.compat.v1.summary.FileWriter.add_summary (also known as tf.summary.FileWriter.add_summary in TensorFlow 1.x)\n",
    "tf.summary.scalar (only in TensorFlow 2.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-9af87942650c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-9af87942650c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <example of custom eval metric>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<example of custom eval metric>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_datetime (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "euclidean (Lambda)              (None,)              0           pickup_longitude[0][0]           \n",
      "                                                                 pickup_latitude[0][0]            \n",
      "                                                                 dropoff_longitude[0][0]          \n",
      "                                                                 dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hourofday (Lambda)              (None,)              0           pickup_datetime[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 130)          1000000     scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 euclidean[0][0]                  \n",
      "                                                                 hourofday[0][0]                  \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "h0 (Dense)                      (None, 32)           4192        dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "h1 (Dense)                      (None, 8)            264         h0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fare (Dense)                    (None, 1)            9           h1[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 1,004,465\n",
      "Trainable params: 1,004,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train for 20 steps\n",
      "\n",
      "Epoch 00001: saving model to ./taxifare-model/checkpoints\n",
      "20/20 - 2s - loss: 238.5746 - rmse: 13.8334 - mse: 238.5746 - val_loss: 199.3960 - val_rmse: 12.8126 - val_mse: 200.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 17:01:46.529586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-03-27 17:01:47.775619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-03-27 17:01:47.777498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "2020-03-27 17:01:48.723672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-03-27 17:01:50.703900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.704643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-03-27 17:01:50.704678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-03-27 17:01:50.704718: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-03-27 17:01:50.706790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-03-27 17:01:50.707145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-03-27 17:01:50.709476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-03-27 17:01:50.710674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-03-27 17:01:50.710730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-03-27 17:01:50.710865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.711596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.712383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-03-27 17:01:50.721033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-03-27 17:01:50.721385: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562514944d70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-03-27 17:01:50.721424: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-03-27 17:01:50.770358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.771291: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5625149bb220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-03-27 17:01:50.771324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-03-27 17:01:50.771582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.772374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-03-27 17:01:50.772435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-03-27 17:01:50.772464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-03-27 17:01:50.772539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-03-27 17:01:50.772574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-03-27 17:01:50.772591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-03-27 17:01:50.772611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-03-27 17:01:50.772630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-03-27 17:01:50.772699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.773452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:50.774132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-03-27 17:01:50.774240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-03-27 17:01:51.463632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-03-27 17:01:51.463693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2020-03-27 17:01:51.463704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2020-03-27 17:01:51.464014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:51.464835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-03-27 17:01:51.465573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10714 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2020-03-27 17:01:53.064427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-03-27 17:01:53.257384: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\n",
      "2020-03-27 17:01:53.257474: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs\n",
      "2020-03-27 17:01:53.258470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\n",
      "2020-03-27 17:01:53.359082: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2020-03-27 17:01:53.359871: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2020-03-27 17:01:53.369308: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\n",
      "2020-03-27 17:01:53.369348: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\n",
      "2020-03-27 17:01:54.011138: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2020-03-27 17:01:54.011331: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2020-03-27 17:01:55.280999: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=./taxifare/tests/data/taxi-valid*\n",
    "TRAIN_DATA_PATH=./taxifare/tests/data/taxi-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "python3 ./taxifare/trainer/task.py \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--nnsize 32 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 216\n",
      "drwxr-xr-x 7 jupyter jupyter  4096 Mar 27 17:25 .\n",
      "drwxr-xr-x 6 jupyter jupyter  4096 Mar 25 13:54 ..\n",
      "-rw-r--r-- 1 jupyter jupyter  6903 Mar 24 21:44 0_export_data_from_bq_to_gcs.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 25390 Mar 27 15:21 1_training_at_scale.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 45383 Mar 27 17:25 2_hyperparameter_tuning.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 14611 Mar 26 17:19 3_kubeflow_pipelines.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 47608 Mar 25 13:54 4a_streaming_data_training.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 18620 Mar 25 13:54 4b_streaming_data_inference.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter   269 Mar 25 17:58 Dockerfile\n",
      "-rw-r--r-- 1 jupyter jupyter   681 Mar 27 16:59 hptuning_config.yaml\n",
      "drwxr-xr-x 2 jupyter jupyter  4096 Mar 27 17:00 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 jupyter jupyter   599 Mar 24 21:44 Makefile\n",
      "drwxr-xr-x 3 jupyter jupyter  4096 Mar 24 21:44 pipelines\n",
      "-rw-r--r-- 1 jupyter jupyter    30 Mar 25 22:16 requirements.txt\n",
      "-rw-r--r-- 1 jupyter jupyter   208 Mar 27 16:49 setup.py\n",
      "drwxr-xr-x 2 jupyter jupyter  4096 Mar 25 13:54 taxicab_traffic\n",
      "drwxr-xr-x 6 jupyter jupyter  4096 Mar 27 17:13 taxifare\n",
      "drwxr-xr-x 4 jupyter jupyter  4096 Mar 27 17:01 taxifare-model\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/trained_model munn-sandbox taxifare_200327_184752\n",
      "jobId: taxifare_200327_184752\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [taxifare_200327_184752] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe taxifare_200327_184752\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs taxifare_200327_184752\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Replace with your BUCKET and REGION\n",
    "BUCKET=\"munn-sandbox\"\n",
    "REGION=\"munn-sandbox\"\n",
    "TFVERSION=\"2.1\"\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model\n",
    "JOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=15\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=10\n",
    "NBUCKETS=10\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://${BUCKET}/taxifare\n",
    "DATA_PATH=${GCS_PROJECT_PATH}/data\n",
    "OUTPUT_DIR=${GCS_PROJECT_PATH}/model_hptune\n",
    "TRAIN_DATA_PATH=${DATA_PATH}/taxi-train*\n",
    "EVAL_DATA_PATH=${DATA_PATH}/taxi-valid*\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=taxifare/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --config=hptuning_config.yaml \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    -- \\\n",
    "    --eval_data_path=${EVAL_DATA_PATH} \\\n",
    "    --output_dir=${OUTPUT_DIR} \\\n",
    "    --train_data_path=${TRAIN_DATA_PATH} \\\n",
    "    --batch_size ${BATCH_SIZE} \\\n",
    "    --num_examples_to_train_on ${NUM_EXAMPLES_TO_TRAIN_ON} \\\n",
    "    --num_evals ${NUM_EVALS} \\\n",
    "    --nbuckets ${NBUCKETS} \\\n",
    "    --nnsize ${NNSIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR=\"gs://{}/taxifare/trained_hp_tune\".format(BUCKET)\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ai-platform jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=taxifaremodel \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --job-dir=gs://{BUCKET}/taxifare \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version={TFVERSION} \\\n",
    "    --region={REGION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-train.csv \\\n",
    "    --eval_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-valid.csv  \\\n",
    "    --train_steps=5000 \\\n",
    "    --output_dir={OUTDIR}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
