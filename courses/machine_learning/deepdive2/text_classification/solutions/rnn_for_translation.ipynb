{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Encode-Decoder for Translation\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to create a tf.data.Dataset for seq2seq problems\n",
    "1. Learn how to train an encoder-decoder model in Keras\n",
    "1. Learn how to save the encoder and the decoder as separate models \n",
    "1. Learn how to piece together the trained encoder and decoder into a translation function\n",
    "1. Learn how to use the BLUE score to evaluate a translation model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab we'll build a translation model from Spanish to English using an encoder-decoder model architecture. We'll benchmark our results using the industry standard BLEU score then deploy for online prediction using an AI Platform custom prediction routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    GRU,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.models import (\n",
    "    load_model,\n",
    "    Model,\n",
    ")\n",
    "\n",
    "import utils_preproc\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "DATA_URL = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. The dataset contains Spanish-English  translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation data stored at: /Users/dherin/.keras/datasets/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin=DATA_URL, extract=True)\n",
    "\n",
    "path_to_file = os.path.join(\n",
    "    os.path.dirname(path_to_zip),\n",
    "    \"spa-eng/spa.txt\"\n",
    ")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep='\\t', header=None, names=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>105399</td>\n",
       "      <td>Is there anything to drink in the refrigerator?</td>\n",
       "      <td>¿Hay algo para beber en el refrigerador?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92723</td>\n",
       "      <td>I cut my little finger peeling potatoes.</td>\n",
       "      <td>Me hice un corte en el meñique pelando papas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48227</td>\n",
       "      <td>Holland is a small country.</td>\n",
       "      <td>Holanda es un país pequeño.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                english  \\\n",
       "105399  Is there anything to drink in the refrigerator?   \n",
       "92723          I cut my little finger peeling potatoes.   \n",
       "48227                       Holland is a small country.   \n",
       "\n",
       "                                              spanish  \n",
       "105399       ¿Hay algo para beber en el refrigerador?  \n",
       "92723   Me hice un corte en el meñique pelando papas.  \n",
       "48227                     Holanda es un país pequeño.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `utils_preproc` package we have written for you,\n",
    "we will use the following functions to pre-process our dataset of sentence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils_preproc.preprocess_sentence()` method does the following:\n",
    "1. Converts sentence lower case\n",
    "2. Adds a space between puncation and words\n",
    "3. Replaces tokens that aren't a-z or punctation with space\n",
    "4. Adds `<start>` and `<end>` tokens\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> no estamos comiendo . <end>',\n",
       " '<start> esta llegando el invierno . <end>',\n",
       " '<start> el invierno se acerca . <end>',\n",
       " '<start> tom no comio nada . <end>',\n",
       " '<start> su pierna mala le impidio ganar la carrera . <end>',\n",
       " '<start> su respuesta es erronea . <end>',\n",
       " '<start> ¿ que tal si damos un paseo despues del almuerzo ? <end>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = [utils_preproc.preprocess_sentence(s) for s in raw]\n",
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Integerizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils_preproc.tokenize()` method does the following:\n",
    "    \n",
    "1. Splits each sentence into a token list\n",
    "1. Maps each token to an integer\n",
    "1. Pads to length of longest sentence \n",
    "\n",
    "It returns an instance of a [Keras Tokenizer](https://keras.io/preprocessing/text/)\n",
    "containing the token-integer mapping along with the integerized sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  8,  9,  3,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 10, 11,  5,  6,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  5,  6, 12, 13,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 14,  4, 15, 16,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  7, 17, 18, 19, 20, 21, 22, 23,  3,  2,  0,  0],\n",
       "       [ 1,  7, 24, 25, 26,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,  2]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integerized, tokenizer = utils_preproc.tokenize(processed)\n",
    "integerized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputed tokenizer can be used to get back the actual works\n",
    "from the integers representing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> no estamos comiendo . <end>',\n",
       " '<start> esta llegando el invierno . <end>',\n",
       " '<start> el invierno se acerca . <end>',\n",
       " '<start> tom no comio nada . <end>',\n",
       " '<start> su pierna mala le impidio ganar la carrera . <end>',\n",
       " '<start> su respuesta es erronea . <end>',\n",
       " '<start> ¿ que tal si damos un paseo despues del almuerzo ? <end>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(integerized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `load_and_preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first implement a function that will read the raw sentence-pair file\n",
    "and preprocess the sentences with `utils_preproc.preprocess_sentence`.\n",
    "\n",
    "The `load_and_preprocess` function takes as input\n",
    "- the path where the sentence-pair file is located\n",
    "- the number of examples one wants to read in\n",
    "\n",
    "It returns a tuple whose first component contains the english\n",
    "preprocessed sentences, while the second component contains the\n",
    "spanish ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(path, num_examples):\n",
    "    with open(path_to_file, 'r') as fp:\n",
    "        lines = fp.read().strip().split('\\n')\n",
    "\n",
    "    # TODO 1a\n",
    "    sentence_pairs = [ \n",
    "        [utils_preproc.preprocess_sentence(sent) for sent in line.split('\\t')]\n",
    "        for line in lines[:num_examples]\n",
    "    ]\n",
    "\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> fire ! <end>\n",
      "<start> incendio ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = load_and_preprocess(path_to_file, num_examples=10)\n",
    "\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `load_and_integerize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `utils_preproc.tokenize`, let us now implement the function `load_and_integerize` that takes as input the data path along with the number of examples we want to read in and returns the following tuple:\n",
    "\n",
    "```python\n",
    "  (input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer)\n",
    "```\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "* `input_tensor` is an integer tensor of shape `(batch_size, max_length_inp)` containing the integerized versions of the source language sentences\n",
    "* `target_tensor` is an integer tensor of shape `(batch_size, max_length_targ)` containing the integerized versions of the target language sentences\n",
    "* `inp_lang_tokenizer` is the source language tokenizer\n",
    "* `targ_lang_tokenizer` is the target language tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_integerize(path, num_examples=None):\n",
    "\n",
    "    targ_lang, inp_lang = load_and_preprocess(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = utils_preproc.tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = utils_preproc.tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split this data 80/20 into train and validation, and we'll use only the first 30K examples, Since we'll be training on a single GPU. \n",
    " \n",
    "Let us set variable for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our integerized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_and_integerize(\n",
    "    path_to_file, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us store the maximal sentence length of both languages into two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ = target_tensor.shape[1]\n",
    "max_length_inp = input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now using scikit-learn `train_test_split` to create our splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=TEST_PROP, random_state=SEED)\n",
    "\n",
    "input_tensor_train = splits[0]\n",
    "input_tensor_val = splits[1]\n",
    "\n",
    "target_tensor_train = splits[2]\n",
    "target_tensor_val = splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(input_tensor_train), len(target_tensor_train),\n",
    " len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; int to word mapping\n",
      "[  1 133  14 316   3   2   0   0   0   0   0   0   0   0   0   0]\n",
      "['<start>', 'deja', 'de', 'leer', '.', '<end>', '', '', '', '', '', '', '', '', '', ''] \n",
      "\n",
      "Target Language; int to word mapping\n",
      "[  1  86 341   3   2   0   0   0   0   0   0]\n",
      "['<start>', 'stop', 'reading', '.', '<end>', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; int to word mapping\")\n",
    "print(input_tensor_train[0])\n",
    "print(utils_preproc.int2word(inp_lang, input_tensor_train[0]), '\\n')\n",
    "\n",
    "print(\"Target Language; int to word mapping\")\n",
    "print(target_tensor_train[0])\n",
    "print(utils_preproc.int2word(targ_lang, target_tensor_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 133,  14, 316,   3,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  1,  49,   7,  19, 448,   3,   2,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  86, 341,   3,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,  19,   8,  21, 519,   3,   2,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset\n",
    "\n",
    "Note how our labels are our reference translations shifted ahead by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_dataset(encoder_input, decoder_input):\n",
    "\n",
    "    # shift ahead by 1\n",
    "    target = tf.roll(decoder_input, -1, 1)\n",
    "\n",
    "    # replace last column with 0s\n",
    "    zeros = tf.zeros([target.shape[0], 1], dtype=tf.int32)\n",
    "    target = tf.concat((target[:, :-1], zeros), axis=-1)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((encoder_input, decoder_input), target))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = encoder_decoder_dataset(\n",
    "    input_tensor_train, target_tensor_train).shuffle(\n",
    "    BUFFER_SIZE).repeat().batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "eval_dataset = encoder_decoder_dataset(\n",
    "    input_tensor_val, target_tensor_val).batch(\n",
    "    BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We use an encoder-decoder decoder architecture, however we embed our words into a latent space prior to feeding them into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "encoder_inputs_embedded = Embedding(\n",
    "    input_dim=vocab_inp_size,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_length_inp)(encoder_inputs)\n",
    "\n",
    "encoder_rnn = GRU(\n",
    "     units=1024,\n",
    "     return_sequences=True,\n",
    "     return_state=True,\n",
    "     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "\n",
    "decoder_inputs_embedded = Embedding(\n",
    "    input_dim=vocab_tar_size,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_length_targ)(decoder_inputs)\n",
    "\n",
    "decoder_rnn = GRU(\n",
    "    units=1024,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "decoder_outputs, decoder_state = decoder_rnn(\n",
    "    decoder_inputs_embedded, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier (take each intermediate hidden state and predict word)\n",
    "\n",
    "decoder_dense = Dense(vocab_tar_size, activation='softmax')\n",
    "\n",
    "predictions = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2409984     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    1263360     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 1024), 3938304     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     [(None, None, 1024), 3938304     embedding_3[0][0]                \n",
      "                                                                 gru_2[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4935)   5058375     gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 16,608,327\n",
      "Trainable params: 16,608,327\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 375 steps, validate for 93 steps\n",
      "Epoch 1/9\n",
      "375/375 [==============================] - 340s 908ms/step - loss: 2.0976 - val_loss: 1.6077\n",
      "Epoch 2/9\n",
      "375/375 [==============================] - 334s 890ms/step - loss: 1.3844 - val_loss: 1.2762\n",
      "Epoch 3/9\n",
      "375/375 [==============================] - 335s 893ms/step - loss: 1.0384 - val_loss: 1.1066\n",
      "Epoch 4/9\n",
      "375/375 [==============================] - 344s 918ms/step - loss: 0.7874 - val_loss: 0.9922\n",
      "Epoch 5/9\n",
      "375/375 [==============================] - 348s 928ms/step - loss: 0.5762 - val_loss: 0.9180\n",
      "Epoch 6/9\n",
      "375/375 [==============================] - 352s 939ms/step - loss: 0.4072 - val_loss: 0.8774\n",
      "Epoch 7/9\n",
      "375/375 [==============================] - 365s 974ms/step - loss: 0.2814 - val_loss: 0.8669\n",
      "Epoch 8/9\n",
      "375/375 [==============================] - 364s 971ms/step - loss: 0.1943 - val_loss: 0.8582\n",
      "Epoch 9/9\n",
      "375/375 [==============================] - 351s 937ms/step - loss: 0.1375 - val_loss: 0.8767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15da12da0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=eval_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We can't just use model.predict(), because we don't know all the inputs we used during training. We only know the encoder_input (source language) but not the decoder_input (target language).\n",
    "\n",
    "We do however know the first token of the decoder input, which is the `<start>` token. So using this plus the state of the encoder RNN, we can predict the next token. We will then use that token to be the second token of decoder input, and continue like this until we predict the `<end>` token, or we reach some defined max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    encoder_model = load_model(os.path.join(MODEL_PATH, 'encoder_model.h5'))\n",
    "    decoder_model = load_model(os.path.join(MODEL_PATH, 'decoder_model.h5'))\n",
    "\n",
    "else:\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_state)\n",
    "\n",
    "    decoder_state_input = Input(shape=(units,), name=\"decoder_state_input\")\n",
    "\n",
    "    # Reuses weights from the decoder_rnn layer\n",
    "    decoder_outputs, decoder_state = decoder_rnn(\n",
    "        decoder_inputs_embedded, initial_state=decoder_state_input)\n",
    "\n",
    "    # Reuses weights from the decoder_dense layer\n",
    "    predictions = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "        inputs=[decoder_inputs, decoder_state_input],\n",
    "        outputs=[predictions, decoder_state]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_seqs, output_tokenizer, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_seqs: int tensor of shape (BATCH_SIZE, SEQ_LEN)\n",
    "    output_tokenizer: Tokenizer used to conver from int to words\n",
    "\n",
    "    Returns translated sentences\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    batch_size = input_seqs.shape[0]\n",
    "    states_value = encoder_model.predict(input_seqs)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = tf.ones([batch_size, 1])\n",
    "\n",
    "    decoded_sentences = [[] for _ in range(batch_size)]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    for i in range(max_decode_length):\n",
    "\n",
    "        output_tokens, decoder_state = decoder_model.predict(\n",
    "            [target_seq, states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :], axis=-1)\n",
    "\n",
    "        tokens = utils_preproc.int2word(output_tokenizer, sampled_token_index)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = tf.expand_dims(tf.constant(sampled_token_index), axis=-1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "INPUT:\n",
      "No estamos comiendo.\n",
      "REFERENCE TRANSLATION:\n",
      "We're not eating.\n",
      "MACHINE TRANSLATION:\n",
      "['we', 're', 'not', 'eating', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Está llegando el invierno.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'coming', '.', '<end>', '', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "El invierno se acerca.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'coming', '.', '<end>', '', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Tom no comio nada.\n",
      "REFERENCE TRANSLATION:\n",
      "Tom ate nothing.\n",
      "MACHINE TRANSLATION:\n",
      "['tom', 'didn', 't', 'eat', 'lunch', '.', '<end>', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Su pierna mala le impidió ganar la carrera.\n",
      "REFERENCE TRANSLATION:\n",
      "His bad leg prevented him from winning the race.\n",
      "MACHINE TRANSLATION:\n",
      "['his', 'mother', 'is', 'out', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Su respuesta es erronea.\n",
      "REFERENCE TRANSLATION:\n",
      "Your answer is wrong.\n",
      "MACHINE TRANSLATION:\n",
      "['his', 'word', 'is', 'law', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "¿Qué tal si damos un paseo después del almuerzo?\n",
      "REFERENCE TRANSLATION:\n",
      "How about going for a walk after lunch?\n",
      "MACHINE TRANSLATION:\n",
      "['how', 'was', 'today', ',', 'too', '?', '<end>', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Tom ate nothing.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"Your answer is wrong.\",\n",
    "    \"How about going for a walk after lunch?\"\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(\n",
    "    utils_preproc.preprocess(sentences, inp_lang),\n",
    "    targ_lang,\n",
    "    max_length_targ\n",
    ")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print('-')\n",
    "    print('INPUT:')\n",
    "    print(sentences[i])\n",
    "    print('REFERENCE TRANSLATION:')\n",
    "    print(reference_translations[i])\n",
    "    print('MACHINE TRANSLATION:')\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Model\n",
    "\n",
    "Save model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_CHECKPOINT:\n",
    "\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    model.save(os.path.join(MODEL_PATH, 'model.h5'))\n",
    "    encoder_model.save(os.path.join(MODEL_PATH, 'encoder_model.h5'))\n",
    "    decoder_model.save(os.path.join(MODEL_PATH, 'decoder_model.h5'))\n",
    "\n",
    "    with open(os.path.join(MODEL_PATH, 'encoder_tokenizer.pkl'), 'wb') as fp:\n",
    "        pickle.dump(inp_lang, fp)\n",
    "\n",
    "    with open(os.path.join(MODEL_PATH, 'decoder_tokenizer.pkl'), 'wb') as fp:\n",
    "        pickle.dump(targ_lang, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation. \n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLEU).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It allows flexibility for the ordering of words and phrases.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "It still is imperfect, since it gives no credit to synonyms and so human evaluation is still best when feasible. However BLEU is commonly considered the best among bad options for an automated metric.\n",
    "\n",
    "The NLTK framework has an implementation that we will use.\n",
    "\n",
    "We can't run calculate BLEU during training, because at that time the correct decoder input is used. Instead we'll calculate it now.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_1(reference, candidate):\n",
    "    reference = list(filter(lambda x: x != '', reference))  # remove padding\n",
    "    candidate = list(filter(lambda x: x != '', candidate))  # remove padding\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "    return nltk.translate.bleu_score.sentence_bleu(\n",
    "        reference, candidate, (1,), smoothing_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_4(reference, candidate):\n",
    "    reference = list(filter(lambda x: x != '', reference))  # remove padding\n",
    "    candidate = list(filter(lambda x: x != '', candidate))   # remove padding\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "    return nltk.translate.bleu_score.sentence_bleu(\n",
    "        reference, candidate, (.25, .25, .25, .25), smoothing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes ~ 5min to run, the bulk of which is decoding the 6000 sentences in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 1: 0.25360982204741905\n",
      "BLEU 4: 0.04659966638405097\n",
      "CPU times: user 8min 46s, sys: 26.6 s, total: 9min 13s\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_examples = len(input_tensor_val)\n",
    "bleu_1_total = 0\n",
    "bleu_4_total = 0\n",
    "\n",
    "\n",
    "for idx in range(num_examples):\n",
    "\n",
    "    reference_sentence = utils_preproc.int2word(\n",
    "        targ_lang, target_tensor_val[idx][1:])\n",
    "\n",
    "    decoded_sentence = decode_sequences(\n",
    "        input_tensor_val[idx:idx+1], targ_lang, max_length_targ)[0]\n",
    "\n",
    "    bleu_1_total += bleu_1(reference_sentence, decoded_sentence)\n",
    "    bleu_4_total += bleu_4(reference_sentence, decoded_sentence)\n",
    "\n",
    "print('BLEU 1: {}'.format(bleu_1_total/num_examples))\n",
    "print('BLEU 4: {}'.format(bleu_4_total/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "- Batch_Size: 64\n",
    "- Optimizer: adam\n",
    "- Embed_dim: 256\n",
    "- GRU Units: 1024\n",
    "- Train Examples: 24,000\n",
    "- Epochs: 10\n",
    "- Hardware: P100 GPU\n",
    "\n",
    "**Performance**\n",
    "- Training Time: 5min \n",
    "- Cross-entropy loss: train: 0.0722 - val: 0.9062\n",
    "- BLEU 1: 0.2519574312515255\n",
    "- BLEU 4: 0.04589972764144636"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "See `translate_deploy.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
