{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TF-Hub Word Embedding with text2hub\n",
    "\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to deploy deploy AI Hub Kubeflow pipeline\n",
    "  1. Learn how to configure the run parameters for text2hub\n",
    "  1. Learn how to inspect text2hub generated artifacts and word embeddings in TensorBoard\n",
    "  1. Learn how to run TF 1.x generated hub module in TF 2.0\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "Pre-trained re-usable text embeddings such as TF-hub module are a great tool for text models, since they capture relationship between words. They are generally trained on vast but generic text corpuses like Wikipedia or Google news, which make them usually very good at representing generic text, but not so much when the text comes from a very specialized domain with words unique to that domain, such as in the medical field.\n",
    "One problem in particular is that applying a TF-hub text modules pre-trained on a generic corpus to a specialized text will send all the unique domain words to the same OOV vector, often the zero vector. By doing so we lose a very valuable part of the text information, because for specialized texts the most informative words are often the words that are very specific to that special domain.\n",
    "Another issue is that of commonly misspelled words from text gather from say customer feedback. Applying a generic pre-trained embedding will send the misspelled word to the OOV vectors, losing precious info. However, creating a TF-hub module tailored to the texts coming from that customer feedback feed will place the commonly misspelling of a given word closeby to the original word in the embedding space. \n",
    "\n",
    "In this notebook, we will learn how to generate a text TF-hub module specific to a particular domain using the text2hub Kubeflow pipeline available on Google AI Hub. This pipeline takes as input a corpus of text stored in a GCS bucket and outputs a TF-Hub module to a GCS bucket. The generated TF-Hub module can then be reused both in TF 1.x or in TF 2.0 code by referencing the output GCS bucket path when loading the module. \n",
    "\n",
    "Our first order of business will be to learn how to deploy a Kubeflow pipeline, namely text2hub, stored in AI Hub to a Kubeflow cluster. Then we will dig into the pipeline run parameter configuration and review the artifacts produced by the pipeline during its run. These artifacts are meant to help you assess how good the domain specific TF-hub module you generated is. In particular, we will  explore the embedding space visually using TensorBoard projector, which provides a tool to list the nearest neighbors to a given word in the embedding space.\n",
    "At last, we will explain how to run the generated module both in TF 1.x and TF 2.0. Running the module in TF 2.0 will necessite a small trick thatâ€™s useful to know in itself because it allows you to use all the TF 1.x modules in TF hub in TF 2.0 as a Keras layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace by your GCP project and bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"<YOUR PROJEC>\"\n",
    "BUCKET = \"<YOUR BUCKET>\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Kubeflow cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have a running Kubeflow cluster. \n",
    "\n",
    "If not, to deploy a [Kubeflow](https://www.kubeflow.org/) cluster in your GCP project, use the [Kubeflow cluster deployer](https://deploy.kubeflow.cloud/#/deploy).\n",
    "\n",
    "There is a [setup video](https://www.kubeflow.org/docs/started/cloud/getting-started-gke/~) that will\n",
    "take you over all the steps in detail, and explains how to access to the Kubeflow Dashboard UI, once it is \n",
    "running. \n",
    "\n",
    "You'll need to create an OAuth client for authentication purposes: Follow the \n",
    "instructions [here](https://www.kubeflow.org/docs/gke/deploy/oauth-setup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset in GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus we chose is one of [Project Gutenberg medical texts](http://www.gutenberg.org/ebooks/bookshelf/48): [A Manual of the Operations of Surgery](http://www.gutenberg.org/ebooks/24564) by Joseph Bell, containing very specialized language. \n",
    "\n",
    "The first thing to do is to upload the text into a GCS bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      " 19  608k   19  116k    0     0   137k      0  0:00:04 --:--:--  0:00:04  137k\r",
      "100  608k  100  608k    0     0   391k      0  0:00:01  0:00:01 --:--:--  391k\n",
      "Copying file://surgery_manual.txt [Content-Type=text/plain]...\n",
      "/ [0 files][    0.0 B/608.3 KiB]                                                \r",
      "/ [1 files][608.3 KiB/608.3 KiB]                                                \r",
      "-\r\n",
      "Operation completed over 1 objects/608.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "URL=http://www.gutenberg.org/cache/epub/24564/pg24564.txt\n",
    "OUTDIR=gs://$BUCKET/custom_embedding\n",
    "CORPUS=surgery_manual.txt\n",
    "\n",
    "curl $URL > $CORPUS\n",
    "gsutil cp $CORPUS $OUTDIR/$CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has very specialized language such as \n",
    "\n",
    "```\n",
    "On the surface of the abdomen the position of this vessel would be \n",
    "indicated by a line drawn from about an inch on either side of the \n",
    "umbilicus to the middle of the space between the symphysis pubis \n",
    "and the crest of the ilium.\n",
    "```\n",
    "\n",
    "Now let's go over the steps involved in creating your own embedding from that corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the `text2hub` pipeline from AI Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go on [AI Hub](https://aihub.cloud.google.com/u/0/) and search for the `text2hub` pipeline, or just follow [this link](https://aihub.cloud.google.com/u/0/p/products%2F4a91d2d0-1fb8-4e79-adf7-a35707071195).\n",
    "You'll land onto a page describing `text2hub`. Click on the \"Download\" button on that page to download the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./assets/text2hub_download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text2hub pipeline is a KubeFlow pipeline that comprises three components; namely:\n",
    "\n",
    "\n",
    "* The **text2cooc** component that computes a word co-occurrence matrix\n",
    "from a corpus of text\n",
    "\n",
    "* The **cooc2emb** component that factorizes the\n",
    "co-occurrence matrix using [Swivel](https://arxiv.org/pdf/1602.02215.pdf) into\n",
    "the word embeddings exported as a tsv file\n",
    "\n",
    "* The **emb2hub** component that takes the word\n",
    "embedding file and generates a TF Hub module from it\n",
    "\n",
    "\n",
    "Each component is implemented as a Docker container image that's stored into Google Cloud Docker registry, [gcr.io](https://cloud.google.com/container-registry/). The `pipeline.tar.gz` file that you downloaded is a yaml description of how these containers need to be composed as well as where to find the corresponding images. \n",
    "\n",
    "**Remark:** Each component can be run individually as a single component pipeline in exactly the same manner as the `text2hub` pipeline. On AI Hub, each component has a pipeline page describing it and from where you can download the associated single component pipeline:\n",
    "\n",
    " * [text2cooc](https://aihub.cloud.google.com/u/0/p/products%2F6d998d56-741e-4154-8400-0b3103f2a9bc)\n",
    " * [cooc2emb](https://aihub.cloud.google.com/u/0/p/products%2Fda367ed9-3d70-4ca6-ad14-fd6bf4a913d9)\n",
    " * [emb2hub](https://aihub.cloud.google.com/u/0/p/products%2F1ef7e52c-5da5-437b-a061-31111ab55312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload the pipeline to the Kubflow cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to your Kubeflow cluster dashboard and click on the pipeline tab to create a new pipeline. You'll be prompted to upload the pipeline file you have just downloaded. Rename the generated pipeline name to be `text2hub` to keep things nice and clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./assets/text2hub_upload.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a pipeline run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the pipeline, you should see `text2hub` appear on the pipeline list. Click on it. This will bring you to a page describing the pipeline (explore!) and allowing you to create a run. You can inspect the input and output parameters of each of the pipeline components by clicking on the component node in the graph representing the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./assets/text2hub_run_creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Enter the run parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text2hub` has the following run parameters you can configure:\n",
    "\n",
    "Argument                                         | Description                                                                           | Optional | Data Type | Accepted values | Default\n",
    "------------------------------------------------ | ------------------------------------------------------------------------------------- | -------- | --------- | --------------- | -------\n",
    "gcs-path-to-the-text-corpus                      | A Cloud Storage location pattern (i.e., glob) where the text corpus will be read from | False    | String    | gs://...        | -\n",
    "gcs-directory-path-for-pipeline-output           | A Cloud Storage directory path where the pipeline output will be exported             | False    | String    | gs://...        | -\n",
    "number-of-epochs                                 | Number of epochs to train the embedding algorithm (Swivel) on                         | True     | Integer   | -               | 40\n",
    "embedding-dimension                              | Number of components of the generated embedding vectors                               | True     | Integer   | -               | 128\n",
    "co-occurrence-word-window-size                   | Size of the sliding word window where co-occurrences are extracted from               | True     | Integer   | -               | 10\n",
    "number-of-out-of-vocabulary-buckets              | Number of out-of-vocabulary buckets                                                   | True     | Integer   | -               | 1\n",
    "minimum-occurrences-for-a-token-to-be-considered | Minimum number of occurrences for a token to be included in the vocabulary            | True     | Integer   | -               | 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can leave most parameters with their default values except for\n",
    "`gcs-path-to-the-test-corpus` whose value should be set to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dherin-sandbox/custom_embedding/surgery_manual.txt\r\n"
     ]
    }
   ],
   "source": [
    "!echo gs://$BUCKET/custom_embedding/surgery_manual.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for `gcs-directory-path-for-pipeline-output` which we will set to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dherin-sandbox/custom_embedding\r\n"
     ]
    }
   ],
   "source": [
    "!echo gs://$BUCKET/custom_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: `gcs-path-to-the-test-corpus` will accept a GCS pattern like `gs://BUCKET/data/*.txt` or simply a path like `gs://BUCKET/data/` to a GCS directory. All the files that match the pattern or that are in that directory will be parsed to create the word embedding TF-Hub module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./assets/text2hub_run_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these values have been set, you can start the run by clicking on \"Start\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect the run artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run has started you can see its state by going to the experiment tab and clicking on the name of the run (here \"text2hub-1\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/text2hub_experiment_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will show you the pipeline graph. The components in green have successfuly completed. You can then click on them and look at the artifacts that these components have produced.\n",
    "\n",
    "The `text2cooc` components has \"co-occurrence extraction summary\" showing you the GCS path where the co-occurrence data has been saved. Their is a corresponding link that you can paste into your browser to inspect the co-occurrence data from the GCS browser. Some statistics about the vocabulary are given to you such as the most and least frequent tokens. You can also download the vocabulary file containing the token to be embedded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/text2cooc_markdown_artifacts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cooc2emb` has three artifacts\n",
    "* An \"Embedding Extraction Summary\" providing the information as where the model chekpoints and the embedding tables are exported to on GCP\n",
    "* A similarity matrix from a random sample of words giving you an indication whether the model associates close-by vectors to similar words\n",
    "* An button to start TensorBoard from the UI to inspect the model and visualize the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/cooc2emb_artifacts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the word embedding visualization provided by TensorBoard. Start TensorBoard by clicking on \"Start\" and then \"Open\" buttons, and then select \"Projector\".\n",
    "\n",
    "**Remark:** The projector tab may take some time to appear. If it takes too long it may be that your Kubeflow cluster is running an incompatible version of TensorBoard (you TB version should be between 1.13 and 1.15). If that's the case, just run Tensorboard from CloudShell or locally by issuing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --port 8080 --logdir gs://dherin-sandbox/custom_embedding/embeddings\r\n"
     ]
    }
   ],
   "source": [
    "!echo tensorboard --port 8080 --logdir gs://$BUCKET/custom_embedding/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projector view will present you with a representation of the word vectors in a 3 dimensional space (the dim is reduced through PCA) that you can interact with. Enter in the search tool a few words like \"ilium\" and points in the 3D space will light up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/cooc2emb_tb_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you click on a word vector, you'll see appear the n nearest neighbors of that word in the embedding space. The nearset neighbors are both visualized in the center panel and presented as a flat list on the right. \n",
    "\n",
    "Explore the nearest neighbors of a given word and see if they kind of make sense. This will give you a rough understanding of the embedding quality. If it nearest neighbors do not make sense after trying for a few key words, you may need rerun `text2hub`, but this time with either more epochs or more data. Reducing the embedding dimension may help as well as modifying the co-occurence window size (choose a size that make sense given how your corpus is split into lines.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/cooc2emb_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emb2hub` artifacts give you a snippet of TensorFlow 1.x code showing you how to re-use the generated TF-Hub module in your code. We will demonstrate how to use the TF-Hub module in TF 2.0 in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](assets/emb2hub_artifacts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Using the generated TF-Hub module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how to load the TF-Hub module generated by `text2hub` in TF 2.0.\n",
    "\n",
    "We first store the GCS bucket path where the TF-Hub module has been exported into a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://dherin-sandbox/custom_embedding/hub-module'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE = \"gs://{bucket}/custom_embedding/hub-module\".format(bucket=BUCKET)\n",
    "MODULE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TF-hub model has been saved in TF 1.x format, it's not callable by default when you load it with `hub.load`. We need a light wrapper class around the result of `hub.load` that turns it into a callable, by overloading the `__call__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(tf.train.Checkpoint):\n",
    "    def __init__(self, spec):\n",
    "        super(Wrapper, self).__init__()\n",
    "        self.module = hub.load(spec)\n",
    "        self.variables = self.module.variables\n",
    "        self.trainable_variables = []\n",
    "    def __call__(self, x):\n",
    "        return self.module.signatures[\"default\"](x)[\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a `KerasLayer` out of our custom text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dherin/Desktop/asl/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "med_embed = hub.KerasLayer(Wrapper(MODULE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That layer when called with a list of sentences will create a sentence vector for each sentence by averaging the word vectors of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=139, shape=(3, 128), dtype=float32, numpy=\n",
       "array([[ 0.14453109,  0.07852267,  0.10940594,  0.46825212,  0.2711016 ,\n",
       "        -0.30434078,  0.35756677, -0.25814775,  0.58705705, -0.33127564,\n",
       "        -0.3224882 , -0.14958593,  0.27588782, -0.16383132, -0.15279762,\n",
       "        -0.37463644, -0.37352383, -0.375445  , -0.41296828, -0.1574907 ,\n",
       "         0.6536843 ,  0.12237123,  0.12104838,  0.1749343 , -0.41965425,\n",
       "        -0.08719758,  0.19248757,  0.01838654, -0.06238181, -0.23054636,\n",
       "         0.03802659, -0.06728458, -0.23047519,  0.4418791 , -0.21132842,\n",
       "        -0.09843157, -0.08029427, -0.11595032,  0.12324606, -0.09030671,\n",
       "         0.3085222 , -0.08089428, -0.24685636, -0.60403794,  0.4301253 ,\n",
       "        -0.46597183,  0.475734  , -0.43883252, -0.11868288, -0.21692485,\n",
       "        -0.01024164, -0.3957712 ,  0.371831  ,  0.30342016,  0.1703456 ,\n",
       "        -0.40179157,  0.48199773,  0.4366476 , -0.1528644 , -0.15868089,\n",
       "         0.3833921 ,  0.22966436,  0.53352064,  0.19017717, -0.20319304,\n",
       "        -0.14291056,  0.27719742,  0.12461181,  0.2685519 , -0.02031964,\n",
       "        -0.31587142, -0.24115059,  0.49385047,  0.46896133,  0.15680353,\n",
       "         0.5757193 , -0.35598013, -0.17011577,  0.37994483,  0.38705346,\n",
       "        -0.48963422, -0.22141391, -0.13311529,  0.1217213 ,  0.10572889,\n",
       "         0.00914867,  0.48309857,  0.19068594,  0.65811855, -0.3488899 ,\n",
       "         0.82147115, -0.12336684, -0.19860533, -0.00717204,  0.00655735,\n",
       "         0.63817227, -0.11087445, -0.09598988,  0.00900863,  0.09973834,\n",
       "        -0.09877969,  0.14547217,  0.03858263, -0.05532194,  0.15113775,\n",
       "         0.10626303,  0.6526023 , -0.25385836, -0.07579204, -0.10374842,\n",
       "        -0.16401999, -0.3356936 ,  0.1496517 ,  0.51793283,  0.23419356,\n",
       "        -0.22008544, -0.13944909, -0.82629   ,  0.32507917, -0.565222  ,\n",
       "        -0.5308213 ,  0.2986781 , -0.2662198 ,  0.69686294, -0.6315541 ,\n",
       "        -0.5368628 , -0.3538828 , -0.12873396],\n",
       "       [ 0.0164449 ,  0.5205895 , -0.10336486,  0.0294515 , -0.71586806,\n",
       "        -0.15337142, -0.34698248,  0.06362253, -0.34007043, -0.47799248,\n",
       "        -0.5058795 , -0.10724854, -0.4296319 ,  0.15193886,  0.0237858 ,\n",
       "         0.33866483,  0.35038096,  0.13515502,  0.08797733,  0.43195003,\n",
       "        -0.08828087,  0.31622517, -0.1284965 , -0.22162655, -0.15961117,\n",
       "         0.17207135, -0.351691  ,  0.46430945, -0.1575633 ,  0.20139848,\n",
       "        -0.12075657,  0.38304433,  0.16132247, -0.01462215,  0.48586917,\n",
       "         0.19514655,  0.9107046 , -0.25045842, -0.06713019, -0.12484521,\n",
       "        -0.06797902, -0.13163768,  0.62818295, -0.3535059 , -0.3789196 ,\n",
       "        -0.26582608,  0.15908894,  0.06220404,  0.32750323, -0.10181555,\n",
       "        -0.22609036,  0.22604214, -0.1598364 , -0.28146088, -0.06265116,\n",
       "         0.48550162,  0.62289774,  0.60894567,  0.10107183, -0.3942115 ,\n",
       "         0.00271538,  0.07179733, -0.33155435, -0.00684002,  0.05756585,\n",
       "        -0.28062728,  0.06826852, -0.10421072,  0.30263296, -0.02316381,\n",
       "         0.4144894 ,  0.03589106,  0.30089313,  0.24118917,  0.16196875,\n",
       "         0.20013857, -0.33502275,  0.04487445, -0.27170265,  0.09177405,\n",
       "         0.12682006, -0.03749305, -0.37236017, -0.17447802, -0.5305285 ,\n",
       "        -0.14423697, -0.0727323 ,  0.04862923, -0.07223414, -0.43564138,\n",
       "         0.10396329, -0.5810602 ,  0.35406083, -0.1256576 ,  0.02135541,\n",
       "         0.245058  ,  0.04531484,  0.6245269 ,  0.39269996, -0.2998444 ,\n",
       "         0.17946783,  0.35652742,  0.3598144 , -0.8541947 , -0.03817117,\n",
       "        -0.03477518, -0.17969337, -0.01614079,  0.2158271 ,  0.28480595,\n",
       "         0.4054087 , -0.28810814,  0.0132064 , -0.24125242,  0.03827119,\n",
       "        -0.02110384, -0.20537867,  0.09987734,  0.1745032 , -0.15917343,\n",
       "        -0.2961843 ,  0.6385125 , -0.18270075,  0.18034573,  0.23292059,\n",
       "         0.06388818,  0.3959585 ,  0.1948332 ],\n",
       "       [ 0.13811088, -0.09591421,  0.31303802,  0.24009922, -0.0812918 ,\n",
       "         0.14325963,  0.06262179, -0.06275751,  0.26495335, -0.48686534,\n",
       "         0.3413443 , -0.28140727, -0.44951075, -0.32282868,  0.13710192,\n",
       "        -0.03942354,  0.26617947, -0.1130475 ,  0.13939439,  0.10550673,\n",
       "         0.19619323, -0.22686051,  0.25844324,  0.06733705, -0.16707568,\n",
       "        -0.15205836, -0.5114491 ,  0.37341952, -0.20279683,  0.4576506 ,\n",
       "        -0.02800272, -0.27989018,  0.33208385,  0.00852232, -0.14005467,\n",
       "         0.11981987,  0.21790147, -0.22426009, -0.06426904, -0.03637813,\n",
       "        -0.1315249 ,  0.23980996, -0.08890792,  0.2795673 ,  0.18043289,\n",
       "         0.30665493, -0.17751847, -0.12178431,  0.31715417, -0.13972795,\n",
       "        -0.234842  ,  0.05617719,  0.20617327,  0.21240275, -0.3236465 ,\n",
       "         0.1804206 ,  0.25617716,  0.6137357 ,  0.29672724,  0.16565354,\n",
       "        -0.37149554,  0.02147242, -0.48856175,  0.12420561,  0.231504  ,\n",
       "        -0.18371941,  0.27795634, -0.5476045 , -0.00926034, -0.27517208,\n",
       "        -0.04792161,  0.11804966, -0.04004014, -0.05059653, -0.24388383,\n",
       "        -0.20765102,  0.3991441 ,  0.1749315 ,  0.21538194, -0.22134262,\n",
       "         0.16286355,  0.14585266,  0.5718665 , -0.21328932,  0.13794544,\n",
       "        -0.06670448, -0.05700678,  0.35419303,  0.43550327,  0.46776128,\n",
       "         0.33836746,  0.23739074,  0.45195872,  0.05455807,  0.42990732,\n",
       "         0.14652824,  0.00105203,  0.3239196 , -0.01887704, -0.11426861,\n",
       "        -0.17127517, -0.08945896,  0.48870897, -0.09813708,  0.44773033,\n",
       "        -0.15237601, -0.02145011, -0.20109749, -0.5017692 , -0.57740414,\n",
       "         0.28072864,  0.24212027, -0.3353082 , -0.19923748, -0.2490241 ,\n",
       "        -0.17450345, -0.22412509, -0.37222236,  0.18634911,  0.30547267,\n",
       "         0.04713549,  0.02006195,  0.0703491 ,  0.33243734, -0.02751613,\n",
       "         0.6161408 , -0.31436777,  0.06119841]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = med_embed(tf.constant(['ilium', 'I have a fracture', 'aneurism']))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
