{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Prediction\n",
    "\n",
    "**Objectives**\n",
    " 1. Build a linear, DNN and CNN model in keras to predict stock market behavior\n",
    " 2. Build a simple RNN model and a multi-layer RNN model in keras\n",
    " 3. Combine RNN and CNN architecture to create a keras model to predict stock market behavior\n",
    " 4. Use the tf.data pipeline to train a time series model on larger dataset\n",
    " \n",
    "In this lab we will build a custom Keras model to predict stock market behavior using the stock market dataset we created in the previous labs. We'll start with a linear, DNN and CNN model \n",
    "\n",
    "Since the features of our model are sequential in nature, we'll next look at how to build various RNN models in keras. We'll start with a simple RNN model and then see how to create a multi-layer RNN in keras. We'll also see how to combine features of 1-dimensional CNNs with a typical RNN architecture. \n",
    "\n",
    "We will be exploring a lot of different model types in this notebook. To keep track of your results, record the accuracy on the validation set in the table here. In machine learning there are rarely any \"one-size-fits-all\" so feel free to test out different hyperparameters (e.g. train steps, regularization, learning rates, optimizers, batch size) for each of the models. Keep track of your model performance in the chart below.\n",
    "\n",
    "|  Model   | Validation Accuracy  |\n",
    "|----------|:---------------:|\n",
    "| Baseline |    0.496         |\n",
    "| Linear   |    --        |\n",
    "| DNN      |    --         |\n",
    "| 1-d CNN  |    --         |\n",
    "| simple RNN  |    --         |\n",
    "| multi-layer RNN  |    --         |\n",
    "| RNN using CNN features  |    --         |\n",
    "| CNN using RNN features  |    --         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries and set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, DenseFeatures,\n",
    "                                     Conv1D, MaxPool1D,\n",
    "                                     Reshape, RNN,\n",
    "                                     LSTM, GRU, Bidirectional)\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"your-gcp-project-here\" # REPLACE WITH YOUR PROJECT NAME\n",
    "BUCKET = \"your-gcp-bucket-here\" # REPLACE WITH YOUR BUCKET\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore time series data\n",
    "\n",
    "We'll start by pulling a small sample of the time series data from Big Query and write some helper functions to clean up the data for modeling. We'll use the data from the `percent_change_sp500` table in BigQuery. The `close_values_prior_260` column contains the close values for any given stock for the previous 260 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "bq_query = '''\n",
    "#standardSQL\n",
    "SELECT\n",
    "  symbol,\n",
    "  Date,\n",
    "  direction,\n",
    "  close_values_prior_260\n",
    "FROM\n",
    "  `stock_market.percent_change_sp500`\n",
    "LIMIT\n",
    "  100\n",
    "'''\n",
    "\n",
    "df_stock_raw = bq.query(bq_query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `clean_data` below does three things:\n",
    " 1. First, we'll remove any inf or NA values\n",
    " 2. Next, we parse the `Date` field to read it as a string.\n",
    " 3. Lstly, we convert the label `direction` into a numberic quantity, mapping 'DOWN' to 0, 'STAY' to 1 and 'UP' to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_df):\n",
    "    \"\"\"Cleans data to prepare for training.\n",
    "\n",
    "    Args:\n",
    "        input_df: Pandas dataframe.\n",
    "    Returns:\n",
    "        Pandas dataframe.\n",
    "    \"\"\"\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # Remove inf/na values.\n",
    "    real_valued_rows = ~(df == np.inf).max(axis=1)\n",
    "    df = df[real_valued_rows].dropna()\n",
    "\n",
    "    # TF doesn't accept datetimes in DataFrame.\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # TF requires numeric label.\n",
    "    df['direction_numeric'] = df['direction'].apply(lambda x: {'DOWN': 0,\n",
    "                                                               'STAY': 1,\n",
    "                                                               'UP': 2}[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = clean_data(df_stock_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and preprocessing\n",
    "\n",
    "Before we begin modeling, we'll preprocess our features by scaling to the z-score. This will ensure that the range of the feature values being fed to the model are comparable and should help with convergence during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_HISTORY_COLUMN = 'close_values_prior_260'\n",
    "COL_NAMES = ['day_' + str(day) for day in range(0, 260)]\n",
    "LABEL = 'direction_numeric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_features(df):\n",
    "    \"\"\"z-scale feature columns of Pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        features: Pandas dataframe.\n",
    "    Returns:\n",
    "        Pandas dataframe with each column standardized according to the\n",
    "        values in that column.\n",
    "    \"\"\"\n",
    "    avg = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - avg)/std\n",
    "\n",
    "\n",
    "def create_features(df, label_name):\n",
    "    \"\"\"Create modeling features and label from Pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas dataframe.\n",
    "        label_name: str, the column name of the label.\n",
    "    Returns:\n",
    "        Pandas dataframe\n",
    "    \"\"\"\n",
    "    # Expand 1 column containing a list of close prices to 260 columns.\n",
    "    time_series_features = df[STOCK_HISTORY_COLUMN].apply(pd.Series)\n",
    "\n",
    "    # Rename columns.\n",
    "    time_series_features.columns = COL_NAMES\n",
    "    time_series_features = _scale_features(time_series_features)\n",
    "\n",
    "    # Concat time series features with static features and label.\n",
    "    label_column = df[LABEL]\n",
    "\n",
    "    return pd.concat([time_series_features,\n",
    "                      label_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = create_features(df_stock, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few examples and see that the preprocessing steps were implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_plot = [0, 1, 9, 5]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "for ix in ix_to_plot:\n",
    "    label = df_features['direction_numeric'].iloc[ix]\n",
    "    example = df_features[COL_NAMES].iloc[ix]\n",
    "    ax = example.plot(label=label, ax=ax)\n",
    "    ax.set_ylabel('scaled price')\n",
    "    ax.set_xlabel('prior days')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train-eval-test split\n",
    "\n",
    "Next, we'll make repeatable splits for our train/validation/test datasets and save these datasets to local csv files. The query below will take a subsample of the entire dataset and then create a 70-15-15 split for the train/validation/test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_split(phase):\n",
    "    \"\"\"Create string to produce train/valid/test splits for a SQL query.\n",
    "\n",
    "    Args:\n",
    "        phase: str, either TRAIN, VALID, or TEST.\n",
    "    Returns:\n",
    "        String.\n",
    "    \"\"\"\n",
    "    floor, ceiling = 0, 80\n",
    "    if phase == 'VALID':\n",
    "        floor, ceiling = 80, 90\n",
    "    elif phase == 'TEST':\n",
    "        floor, ceiling = 90, 100\n",
    "    return '''\n",
    "    AND MOD(ABS(FARM_FINGERPRINT(symbol)), EVERY_N * 100) >= (EVERY_N * {0})\n",
    "    AND MOD(ABS(FARM_FINGERPRINT(symbol)), EVERY_N * 100) < (EVERY_N * {1})\n",
    "    '''.format(floor, ceiling)\n",
    "\n",
    "\n",
    "def create_query(phase, sample_size):\n",
    "    \"\"\"Create SQL query to create train/valid/test splits on subsample.\n",
    "\n",
    "    Args:\n",
    "        phase: str, either TRAIN, VALID, or TEST.\n",
    "        sample_size: str, amount of data to take for subsample.\n",
    "    Returns:\n",
    "        String.\n",
    "    \"\"\"\n",
    "    basequery = \"\"\"\n",
    "    #standardSQL\n",
    "    SELECT\n",
    "      symbol,\n",
    "      Date,\n",
    "      direction,\n",
    "      close_values_prior_260\n",
    "    FROM\n",
    "      `stock_market.percent_change_sp500`\n",
    "    WHERE MOD(ABS(FARM_FINGERPRINT(CAST(Date AS STRING))), EVERY_N) = 0\n",
    "    \"\"\"\n",
    "    query = basequery + _create_split(phase)\n",
    "    return query.replace(\"EVERY_N\", sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "for phase in ['TRAIN', 'VALID', 'TEST']:\n",
    "    # 1. Create query string\n",
    "    query_string = create_query(phase, '500')\n",
    "    # 2. Load results into DataFrame\n",
    "    df = bq.query(query_string).to_dataframe()\n",
    "\n",
    "    # 3. Clean, preprocess dataframe\n",
    "    df = clean_data(df)\n",
    "    df = create_features(df, label_name='direction_numeric')\n",
    "\n",
    "    # 3. Write DataFrame to CSV\n",
    "    if not os.path.exists('./data'):\n",
    "        os.mkdir('./data')\n",
    "    df.to_csv('./data/stock-{}.csv'.format(phase.lower()),\n",
    "              index_label=False, index=False)\n",
    "    print(\"Wrote {} lines to {}\".format(\n",
    "        len(df),\n",
    "        './data/stock-{}.csv'.format(phase.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -la ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For experimentation purposes, we'll train various models using data we can fit in memory using the `.csv` files we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIME_STEPS = 260\n",
    "N_LABELS = 3\n",
    "\n",
    "Xtrain = pd.read_csv('data/stock-train.csv')\n",
    "Xvalid = pd.read_csv('data/stock-valid.csv')\n",
    "\n",
    "ytrain = Xtrain.pop(LABEL)\n",
    "yvalid  = Xvalid.pop(LABEL)\n",
    "\n",
    "ytrain_categorical = to_categorical(ytrain.values)\n",
    "yvalid_categorical = to_categorical(yvalid.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor training progress and compare evaluation metrics for different models, we'll use the function below to plot metrics captured from the training job such as training and validation loss or accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(train_data, val_data, label='Accuracy'):\n",
    "    \"\"\"Plot training and validation metrics on single axis.\n",
    "\n",
    "    Args:\n",
    "        train_data: list, metrics obtrained from training data.\n",
    "        val_data: list, metrics obtained from validation data.\n",
    "        label: str, title and label for plot.\n",
    "    Returns:\n",
    "        Matplotlib plot.\n",
    "    \"\"\"\n",
    "    plt.plot(np.arange(len(train_data)) + 0.5,\n",
    "             train_data,\n",
    "             \"b.-\", label=\"Training \" + label)\n",
    "    plt.plot(np.arange(len(val_data)) + 1,\n",
    "             val_data, \"r.-\",\n",
    "             label=\"Validation \" + label)\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(label)\n",
    "    plt.grid(True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Before we begin modeling in keras, let's create a benchmark using a simple heuristic. Let's see what kind of accuracy we would get on the validation set if we predict the majority class of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(yvalid == ytrain.value_counts().idxmax())/yvalid.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So just naively guessing the most common outcome `STAY` will give about 49.6% accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "\n",
    "We'll start with a simple linear model, mapping our sequential input to a single fully dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 1a\n",
    "model = Sequential()\n",
    "model.add(Dense(units=N_LABELS,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=100,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy seems to level out pretty quickly. To report the accuracy of the linear model, we'll average the accuracy on the validation set across the last 20 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our linear model isn't bad but it doesn't beat our benchmark. Perhaps we can do better with a more complicated model. Next, we'll create a deep neural network with keras. We'll experiment with a two layer DNN here but feel free to try a more complex model or add any other addition techinques to try an improve your performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 1b\n",
    "dnn_hidden_units = [16, 8]\n",
    "\n",
    "model = Sequential()\n",
    "for layer in dnn_hidden_units:\n",
    "    model.add(Dense(units=layer,\n",
    "                    activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(units=N_LABELS,\n",
    "                activation=\"softmax\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=50,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "The DNN does slightly better. Let's see how a convolutional neural network performs. \n",
    "\n",
    "A 1-dimensional convolutional can be useful for extracting features from sequential data or deriving features from shorter, fixed-length segments of the data set. Check out the documentation for how to implement a [Conv1d in tensorflow.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D). Max pooling is a downsampling strategy commonly used in conjunction with convolutional neural networks. Next, we'll build a CNN model in keras using the `Conv1D` to create convolution layers and `MaxPool1D` to perform max pooling before passing to a fully connected dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 1c\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "model.add(Conv1D(filters=5,\n",
    "                 kernel_size=5,\n",
    "                 strides=2,\n",
    "                 padding=\"valid\",\n",
    "                 input_shape=[None, 1]))\n",
    "model.add(MaxPool1D(pool_size=2,\n",
    "                    strides=None,\n",
    "                    padding='valid'))\n",
    "\n",
    "\n",
    "# Flatten the result and pass through DNN.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(Dense(units=N_TIME_STEPS//4,\n",
    "                activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(units=N_LABELS, \n",
    "                activation=\"softmax\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=50,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "RNNs are particularly well-suited for learning sequential data. They retain state information from one iteration to the next by feeding the output from one cell as input for the next step. In the cell below, we'll build a RNN model in keras. The final state of the RNN is captured and then passed through a fully connected layer to produce a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 3a\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape inputs to pass through RNN layer.\n",
    "model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "model.add(LSTM(N_TIME_STEPS//8, return_sequences=False))\n",
    "\n",
    "model.add(Dense(units=N_LABELS,\n",
    "                activation='softmax',\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "# Create the model.\n",
    "model.compile(optimizer=Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=50,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer RNN\n",
    "\n",
    "Next, we'll build multi-layer RNN. Just as multiple layers of a deep neural network allow for more complicated features to be learned during training, additional RNN layers can potentially learn complex features in sequential data. For a multi-layer RNN the output of the first RNN layer is fed as the input into the next RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 3b\n",
    "rnn_hidden_units = [N_TIME_STEPS//16,\n",
    "                    N_TIME_STEPS//32]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape inputs to pass through RNN layer.\n",
    "model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "\n",
    "for layer in rnn_hidden_units[:-1]:\n",
    "    model.add(GRU(units=layer,\n",
    "                  return_sequences=True))\n",
    "\n",
    "model.add(GRU(units=rnn_hidden_units[-1],\n",
    "              return_sequences=False))\n",
    "model.add(Dense(units=N_LABELS,\n",
    "                activation=\"softmax\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=50,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CNN and RNN architecture\n",
    "\n",
    "Finally, we'll look at some model architectures which combine aspects of both convolutional and recurrant networks. For example, we can use a 1-dimensional convolution layer to process our sequences and create features which are then passed to a RNN model before prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 4a\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape inputs for convolutional layer\n",
    "model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "\n",
    "model.add(Conv1D(filters=20,\n",
    "                 kernel_size=4,\n",
    "                 strides=2,\n",
    "                 padding=\"valid\",\n",
    "                 input_shape=[None, 1]))\n",
    "model.add(MaxPool1D(pool_size=2,\n",
    "                    strides=None,\n",
    "                    padding='valid'))\n",
    "\n",
    "model.add(LSTM(units=N_TIME_STEPS//2,\n",
    "               return_sequences=False,\n",
    "               kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "model.add(Dense(units=N_LABELS, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=30,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try building a hybrid model which uses a 1-dimensional CNN to create features from the outputs of an RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 4b\n",
    "rnn_hidden_units = [N_TIME_STEPS//32,\n",
    "                    N_TIME_STEPS//64]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape inputs and pass through RNN layer.\n",
    "model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "for layer in rnn_hidden_units:\n",
    "    model.add(LSTM(layer, return_sequences=True))\n",
    "\n",
    "# Apply 1d convolution to RNN outputs.\n",
    "model.add(Conv1D(filters=5,\n",
    "                 kernel_size=3,\n",
    "                 strides=2,\n",
    "                 padding=\"valid\"))\n",
    "model.add(MaxPool1D(pool_size=4,\n",
    "                    strides=None,\n",
    "                    padding='valid'))\n",
    "\n",
    "# Flatten the convolution output and pass through DNN.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(Dense(units=N_TIME_STEPS//32,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "model.add(Dense(units=N_LABELS,\n",
    "                activation=\"softmax\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=Xtrain.values,\n",
    "                    y=ytrain_categorical,\n",
    "                    batch_size=Xtrain.shape[0],\n",
    "                    validation_data=(Xvalid.values, yvalid_categorical),\n",
    "                    epochs=80,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history.history['val_accuracy'][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on larger dataset using tf.data pipeline\n",
    "\n",
    "After exploring various model architectures using a small subsample of the large dataset, we can then train a model using more of the data. To sample all of the data, we can use the code we have above setting `subsample` to be `1` when calling `create_query`. Note also, we'll rename the `.csv` files that we create. \n",
    "\n",
    "Note: Training on the full data set within this notebook will take substantially longer!\n",
    "\n",
    "```pytyon\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "for phase in ['TRAIN', 'VALID', 'TEST']:\n",
    "    # 1. Create query string\n",
    "    query_string = create_query(phase, '1')\n",
    "    # 2. Load results into DataFrame\n",
    "    df = bq.query(query_string).to_dataframe()\n",
    "\n",
    "    # 3. Clean, preprocess dataframe\n",
    "    df = clean_data(df)\n",
    "    df = create_features(df, label_name='direction_numeric')\n",
    "\n",
    "    # 3. Write DataFrame to CSV\n",
    "    if not os.path.exists('./data'):\n",
    "        os.mkdir('./data')\n",
    "    df.to_csv('./data/large-stock-{}.csv'.format(phase.lower()),\n",
    "              index_label=False, index=False)\n",
    "    print(\"Wrote {} lines to {}\".format(\n",
    "        len(df),\n",
    "        './data/large-stock-{}.csv'.format(phase.lower())))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 5a\n",
    "CSV_COLUMNS = COL_NAMES + [LABEL]\n",
    "DEFAULTS = [[0.0]]*len(COL_NAMES) + [[0]]\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    label = row_data.pop(LABEL)\n",
    "\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: tf.estimator.ModeKeys to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"\n",
    "    # Make a CSV dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS)\n",
    "\n",
    "    # Map dataset to features and label\n",
    "    dataset = dataset.map(map_func=features_and_labels, num_parallel_calls=5)\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=2*batch_size).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 5b\n",
    "# Create input layer of feature columns\n",
    "feature_columns = {\n",
    "    colname: tf.feature_column.numeric_column(colname)\n",
    "    for colname in COL_NAMES\n",
    "    }\n",
    "\n",
    "\n",
    "def build_rnn_model():\n",
    "    \"\"\"Create RNN model.\n",
    "\n",
    "    Args: None\n",
    "    Returns: A tf.Keras model.\n",
    "    \"\"\"\n",
    "    # Instatiate sequential keras model.\n",
    "    model = Sequential()\n",
    "    model.add(DenseFeatures(feature_columns=feature_columns.values()))\n",
    "\n",
    "    # Reshape inputs to pass through RNN layer.\n",
    "    model.add(Reshape(target_shape=[N_TIME_STEPS, 1]))\n",
    "    model.add(LSTM(N_TIME_STEPS//32, return_sequences=False))\n",
    "\n",
    "    model.add(Dense(units=N_LABELS,\n",
    "                    activation='softmax',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l1(l=0.1)))\n",
    "\n",
    "    # Create the model.\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up our training job, let's see how many examples we have in our train, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ./data/large-stock*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 500\n",
    "NUM_TRAIN_EXAMPLES = 2250385\n",
    "\n",
    "N_TIME_STEPS = 260\n",
    "N_LABELS = 3\n",
    "\n",
    "LOGDIR = './stock_trained'\n",
    "shutil.rmtree(path=LOGDIR, ignore_errors=True)\n",
    "\n",
    "get_train = create_dataset(pattern='./data/large-stock-train.csv',\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "get_valid = create_dataset(pattern='./data/large-stock-valid.csv',\n",
    "                           batch_size=1000,\n",
    "                           mode=tf.estimator.ModeKeys.EVAL).take(10)\n",
    "\n",
    "callbacks = [\n",
    "  # Save model after every epoch\n",
    "  ModelCheckpoint(filepath=LOGDIR, monitor='val_accuracy', save_freq='epoch'),\n",
    "  # Write TensorBoard logs output directory\n",
    "  TensorBoard(log_dir=LOGDIR)\n",
    "]\n",
    "\n",
    "model = build_rnn_model()\n",
    "\n",
    "history = model.fit(x=get_train,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    steps_per_epoch=NUM_TRAIN_EXAMPLES//BATCH_SIZE,\n",
    "                    validation_data=get_valid,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['loss'],\n",
    "            history.history['val_loss'],\n",
    "            label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history.history['accuracy'],\n",
    "            history.history['val_accuracy'],\n",
    "            label='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
