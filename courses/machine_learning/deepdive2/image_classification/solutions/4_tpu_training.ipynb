{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn how to take [full advantage of a TPU](https://www.tensorflow.org/guide/tpu)\n",
    "[TPU strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy?version=nightly)\n",
    "\n",
    "Setting up the [Cluster Resolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver)\n",
    "\n",
    "Run below in cloud shell to set up TPU\n",
    "`ctpu up --zone=us-central1-b  --tf-version=2.1 --name=my_tpu`\n",
    "\n",
    "It should automatically SSH into the TPU, but alternatively can be accessed from [Compute Engine Interface](https://console.cloud.google.com/compute/instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tpu_models/trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tpu_models/trainer/util.py\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# 10 is a magic number tuned for local training of this dataset.\n",
    "SHUFFLE_BUFFER = 10 * BATCH_SIZE\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "CLASS_NAMES = ['roses', 'sunflowers', 'tulips', 'dandelion', 'daisy']\n",
    "\n",
    "VALIDATION_IMAGES = 370\n",
    "VALIDATION_STEPS = VALIDATION_IMAGES // BATCH_SIZE\n",
    "\n",
    "CROP_SCALING = [IMG_HEIGHT + 10, IMG_WIDTH + 10]\n",
    "MAX_DELTA = 63.0 / 255.0\n",
    "CONTRAST_LOWER = 0.2\n",
    "CONTRAST_UPPER = 1.8\n",
    "\n",
    "\n",
    "def decode_img(img, reshape_dims):\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "\n",
    "def decode_csv(csv_row):\n",
    "    record_defaults = [\"path\", \"flower\"]\n",
    "    filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
    "    image_bytes = tf.io.read_file(filename=filename)\n",
    "    label = tf.math.equal(CLASS_NAMES, label_string)\n",
    "    return image_bytes, label\n",
    "\n",
    "\n",
    "def read_and_preprocess(image_bytes, label, random_augment=False):\n",
    "    if random_augment:\n",
    "        img = decode_img(image_bytes, CROP_SCALING)\n",
    "        img = tf.image.random_crop(img, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, MAX_DELTA)\n",
    "        img = tf.image.random_contrast(img, CONTRAST_LOWER, CONTRAST_UPPER)\n",
    "    else:\n",
    "        img = decode_img(image_bytes, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def read_and_preprocess_with_augment(image_bytes, label):\n",
    "    return read_and_preprocess(image_bytes, label, random_augment=True)\n",
    "\n",
    "\n",
    "def load_dataset(csv_of_filenames, training=True):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_of_filenames) \\\n",
    "        .map(decode_csv).cache()\n",
    "\n",
    "    if training:\n",
    "        dataset = dataset \\\n",
    "            .map(read_and_preprocess_with_augment) \\\n",
    "            .shuffle(SHUFFLE_BUFFER) \\\n",
    "            .repeat(count=None)\n",
    "    else:\n",
    "        dataset = dataset \\\n",
    "            .map(read_and_preprocess) \\\n",
    "            .repeat(count=1)\n",
    "\n",
    "    # Prefetch prepares the next set of batches while current batch is in use.\n",
    "    return dataset.batch(batch_size=BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tpu_models/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tpu_models/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from . import util\n",
    "\n",
    "NCLASSES = len(util.CLASS_NAMES)\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT = .2\n",
    "\n",
    "\n",
    "def build_model(output_dir):\n",
    "    \"\"\"Compiles keras model for image classification.\"\"\"\n",
    "    module_selection = \"mobilenet_v2_100_224\"\n",
    "    module_handle = \"https://tfhub.dev/google/imagenet/{}/feature_vector/4\" \\\n",
    "        .format(module_selection)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        hub.KerasLayer(module_handle, trainable=False),\n",
    "        tf.keras.layers.Dropout(rate=DROPOUT),\n",
    "        tf.keras.layers.Dense(\n",
    "            NCLASSES,\n",
    "            activation='softmax',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(LEARNING_RATE))\n",
    "    ])\n",
    "    model.build((None,)+(util.IMG_HEIGHT, util.IMG_WIDTH, util.IMG_CHANNELS))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model, num_epochs, steps_per_epoch, train_data, eval_data, output_dir):\n",
    "    \"\"\"Compiles keras model and loads data into it for training.\"\"\"\n",
    "    callbacks = []\n",
    "    if output_dir:\n",
    "        tensorboard_callback = TensorBoard(log_dir=output_dir)\n",
    "        callbacks = [tensorboard_callback]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=eval_data,\n",
    "        validation_steps=util.VALIDATION_STEPS,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    if output_dir:\n",
    "        export_path = os.path.join(output_dir, 'keras_export')\n",
    "        model.save(export_path, save_format='tf')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tpu_models/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tpu_models/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "\n",
    "def _parse_arguments(argv):\n",
    "    \"\"\"Parses command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        help='The number of epochs to train',\n",
    "        type=int, default=10)\n",
    "    parser.add_argument(\n",
    "        '--steps_per_epoch',\n",
    "        help='The number of steps per epoch to train',\n",
    "        type=int, default=500)\n",
    "    parser.add_argument(\n",
    "        '--train_path',\n",
    "        help='The path to the training data',\n",
    "        type=str, default=\"gs://cloud-ml-data/img/flower_photos/train_set.csv\")\n",
    "    parser.add_argument(\n",
    "        '--eval_path',\n",
    "        help='The path to the evaluation data',\n",
    "        type=str, default=\"gs://cloud-ml-data/img/flower_photos/eval_set.csv\")\n",
    "    parser.add_argument(\n",
    "        '--tpu_address',\n",
    "        help='The path to the evaluation data',\n",
    "        type=str, required=True)\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='Directory where to save the given model',\n",
    "        type=str, default='tpu_models/')\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Parses command line arguments and kicks off model training.\"\"\"\n",
    "    args = _parse_arguments(sys.argv[1:])[0]\n",
    "    \n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "        tpu=args.tpu_address)\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        train_data = util.load_dataset(args.train_path)\n",
    "        eval_data = util.load_dataset(args.eval_path, training=False)\n",
    "        image_model = model.build_model(args.job_dir)\n",
    "\n",
    "    model_history = model.train_and_evaluate(\n",
    "        image_model, args.epochs, args.steps_per_epoch,\n",
    "        train_data, eval_data, args.job_dir)\n",
    "    print(\"done!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use below to copy model code from this notebook to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://ddetering-experimental/tpu_models/trainer/__init__.py#1584499666535273...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/__pycache__/__init__.cpython-35.pyc#1584499667482608...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/__pycache__/model.cpython-35.pyc#1584499667673468...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/__pycache__/task.cpython-35.pyc#1584499667108376...\n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/__pycache__/util.cpython-35.pyc#1584499667299167...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/model.py#1584499666355621...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/task.py#1584499666723502...\n",
      "Removing gs://ddetering-experimental/tpu_models/trainer/util.py#1584499666914036...\n",
      "/ [8 objects]                                                                   \n",
      "Operation completed over 8 objects.                                              \n",
      "Copying file://tpu_models/trainer/model.py [Content-Type=text/x-python]...\n",
      "Copying file://tpu_models/trainer/__init__.py [Content-Type=text/x-python]...   \n",
      "Copying file://tpu_models/trainer/task.py [Content-Type=text/x-python]...       \n",
      "Copying file://tpu_models/trainer/util.py [Content-Type=text/x-python]...       \n",
      "- [4 files][  5.8 KiB/  5.8 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://tpu_models/trainer/__pycache__/task.cpython-35.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://tpu_models/trainer/__pycache__/util.cpython-35.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://tpu_models/trainer/__pycache__/__init__.cpython-35.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://tpu_models/trainer/__pycache__/model.cpython-35.pyc [Content-Type=application/x-python-code]...\n",
      "\\ [8 files][ 12.3 KiB/ 12.3 KiB]                                                \n",
      "Operation completed over 8 objects/12.3 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil rm -r gs://ddetering-experimental/tpu_models\n",
    "!gsutil cp -r tpu_models gs://ddetering-experimental/tpu_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below on the TPU to copy the model from GCS to the TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil cp -r gs://ddetering-experimental/tpu_models ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below command on TPU to kick off training of the model. Output directory must be a GCS storage bucket as TPUs have a [restricted local file system](https://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem). The first few epochs will be slow as the tensorflow graph is built, but the rest will be very fast. Wheeee!!\n",
    "\n",
    "The UI is a little laggy in showing the epoch runs. It takes about 10 minutes for it to really start flying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m tpu_models.trainer.task \\\n",
    "    --tpu_address=my_tpu \\\n",
    "    --job-dir=gs://ddetering-experimental/flowers_tpu_$(date -u +%y%m%d_%H%M%S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
